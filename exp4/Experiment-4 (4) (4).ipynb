{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc2f8297",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all required packages\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SparkSession,SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import StopWordsRemover, RegexTokenizer,HashingTF\n",
    "from pyspark.ml import Pipeline\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.mllib.clustering import LDA\n",
    "from pyspark.ml.feature import IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe8470e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/08/04 22:58:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Initialise Spark Session\n",
    "spark = SparkSession.builder.appName(\"Experiment4\")\\\n",
    "                            .config(\"spark.sql.broadcastTimeout\", \"36000\")\\\n",
    "                            .config(\"spark.driver.memory\", \"20g\")\\\n",
    "                            .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90a85afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPARING DATAFRAMES FOR DATSETS\n",
    "\n",
    "#Authors Dataframe\n",
    "#df_authors = spark.read.csv(\"Datasets/authors.csv\", sep = \",\", header = True, quote = '\"')\n",
    "\n",
    "#PaperCsv dataframe\n",
    "papersCsvSchema = StructType([\n",
    "    StructField(\"paper_id\",StringType(),False),\n",
    "    StructField(\"type\",StringType(),False),\n",
    "    StructField(\"journal\",StringType(),False),\n",
    "    StructField(\"book_title\",StringType(),False),\n",
    "    StructField(\"series\",StringType(),False),\n",
    "    StructField(\"publisher\",StringType(),False),\n",
    "    StructField(\"pages\",StringType(),False),\n",
    "    StructField(\"volume\",StringType(),False),\n",
    "    StructField(\"number\",StringType(),False),\n",
    "    StructField(\"year\",StringType(),False),\n",
    "    StructField(\"month\",StringType(),False),\n",
    "    StructField(\"postedat\",StringType(),False),\n",
    "    StructField(\"address\",StringType(),False),\n",
    "    StructField(\"title\",StringType(),False),\n",
    "    StructField(\"abstract\",StringType(),False),\n",
    "])\n",
    "df_paperCsv = spark.read.csv(\"Datasets/papers.csv\", sep = \",\", header = False, schema = papersCsvSchema, quote = '\"')\n",
    "\n",
    "#UserLibrary dataframe\n",
    "userLibrarySchema = StructType([\n",
    "    StructField(\"user_hash_id\",StringType(),False),\n",
    "    StructField(\"user_library\",StringType(),False)\n",
    "])\n",
    "df_userLibrary = spark.read.csv(\"Datasets/users_libraries.txt\", sep = \";\", header = False, schema = userLibrarySchema)\n",
    "df_userLibrary = df_userLibrary.selectExpr(\"user_hash_id\",\"split(user_library,',') AS user_library\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d72406e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Cleaning and Tokenizing the data\n",
    "def phraseTokenization(x):\n",
    "    rawPhrase = x[13] + \" \" + x[14] #concatenating title and abstract\n",
    "    rawPhrase = rawPhrase.replace(\"-\",\"\") #removing - from phrase\n",
    "    rawPhrase = rawPhrase.replace(\"_\",\"\") #removing _ from phrase\n",
    "    rawPhrase = rawPhrase.strip() #removing any trailing or leading whitespaces\n",
    "    \n",
    "    #spliting phrase based on non-alphaNumeric characters\n",
    "    phraseArray = re.split('[^a-zA-Z0-9]+',rawPhrase) \n",
    "    \n",
    "    #remove words with less than 3 char\n",
    "    phraseArrayFilteredWords = [i for i in phraseArray if len(i) >= 3]\n",
    "    \n",
    "    return (x[0],list(phraseArrayFilteredWords))\n",
    "\n",
    "\n",
    "df_tokenize = df_paperCsv.na.fill(value=\"\").rdd.map(phraseTokenization).toDF()\n",
    "\n",
    "#Removing StopWords using ML\n",
    "swRemover = StopWordsRemover(inputCol=\"_2\", outputCol=\"cleaned_terms\")\n",
    "df_cleanedData = swRemover.transform(df_tokenize)\n",
    "df_cleanedData = df_cleanedData.selectExpr(\"_1 AS paper_id\",\"cleaned_terms\")\n",
    "\n",
    "#Stemming using Porter stemmer Algo\n",
    "ps =  PorterStemmer()\n",
    "\n",
    "def stemmingTerms(x):\n",
    "    stemmedWords = []\n",
    "    for word in x:\n",
    "        rootWord = ps.stem(word)\n",
    "        stemmedWords.append(rootWord)\n",
    "    return stemmedWords\n",
    "\n",
    "df_cleanedData = df_cleanedData.rdd.mapValues(stemmingTerms).toDF().selectExpr(\"_1 AS paper_id\",\"_2 AS cleaned_terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75cdb7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Find the count of papers in which the term is present\n",
    "df_paperCount = df_cleanedData.selectExpr(\"paper_id\",\"explode(cleaned_terms) AS terms\").distinct().groupBy(\"terms\").count().withColumnRenamed(\"count\", \"paper_count\")\n",
    "\n",
    "#10 percent of total papers present in file\n",
    "noOfDistinctPapers_df = int(df_cleanedData.select(countDistinct(\"paper_id\")).collect()[0][0])\n",
    "tenPercentOfTotalPapers = int(noOfDistinctPapers_df/10)\n",
    "\n",
    "# remove words appear in more than 10% of the papers and keep only the words that appear in at least 20 papers \n",
    "df_filterdTerms = df_paperCount.filter((df_paperCount[\"paper_count\"]<=tenPercentOfTotalPapers) & (df_paperCount[\"paper_count\"]>=20))\n",
    "\n",
    "#Fetch top 1000 terms \n",
    "top1000Terms = df_filterdTerms.orderBy(col(\"paper_count\").desc()).limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "945bfff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/04 22:59:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "21/08/04 23:00:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#associate unique integer values to each term\n",
    "df_termsWithUniqueIndex = top1000Terms.withColumn(\"unique_index\",row_number().over(Window.orderBy(\"paper_count\"))).selectExpr(\"terms\",\"unique_index-1 AS unique_index\")\n",
    "\n",
    "#Collect all terms in a list\n",
    "terms_collection =  [row.terms for row in df_termsWithUniqueIndex.collect()]\n",
    "\n",
    "# Generating Termfrequency Vector for each paper\n",
    "df_cleanedDataExplode = df_cleanedData.selectExpr(\"paper_id\",\"explode(cleaned_terms) AS terms\")\n",
    "\n",
    "#Getting the Unique_index of term \n",
    "df_cleanedDataJoinIndex = df_cleanedDataExplode.join(df_termsWithUniqueIndex,df_cleanedDataExplode.terms == df_termsWithUniqueIndex.terms , how = \"inner\").select(df_cleanedDataExplode.paper_id,df_cleanedDataExplode.terms,df_termsWithUniqueIndex.unique_index)\n",
    "\n",
    "#Getting the term_frequency in each paper\n",
    "df_cleanedDataJoinIndex = df_cleanedDataJoinIndex.groupBy(\"paper_id\",\"unique_index\").count().withColumnRenamed(\"count\", \"term_frquency\")\n",
    "\n",
    "#Creating a sparseVector respresentation for each paper\n",
    "rdd_CleanedDataReducedByPaperId = df_cleanedDataJoinIndex.rdd.map(lambda x: (x[0], [(x[1], x[2])])).reduceByKey(lambda a, b: a + b)\n",
    "rdd_CleanedDataReducedByPaperId = rdd_CleanedDataReducedByPaperId.map(lambda x: (x[0],Vectors.sparse(1000,x[1])))\n",
    "\n",
    "df_CleanedDataSparseVector = rdd_CleanedDataReducedByPaperId.toDF().selectExpr(\"_1 AS paper_id\",\"_2 AS term_frequency_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ede0195",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(paper_id='498902', term_frequency_vector=SparseVector(1000, {33: 3.0, 47: 1.0, 79: 1.0, 97: 1.0, 138: 1.0, 170: 6.0, 354: 1.0, 368: 1.0, 394: 1.0, 482: 2.0, 491: 1.0, 541: 1.0, 550: 1.0, 566: 1.0, 581: 1.0, 596: 1.0, 622: 1.0, 632: 1.0, 663: 1.0, 670: 1.0, 720: 2.0, 723: 1.0, 762: 1.0, 764: 1.0, 773: 1.0, 797: 1.0, 820: 1.0, 826: 1.0, 837: 2.0, 843: 1.0, 879: 1.0, 881: 1.0, 890: 1.0, 892: 1.0, 894: 1.0, 928: 1.0, 937: 1.0, 946: 2.0, 949: 3.0, 952: 1.0, 962: 1.0, 965: 1.0, 984: 1.0, 985: 4.0, 990: 1.0, 992: 1.0}))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_CleanedDataSparseVector.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "838aab53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Ex 3.2\n",
    "\n",
    "#TF-IDF Representation for each paper\n",
    "\n",
    "idf = IDF(inputCol=\"term_frequency_vector\", outputCol=\"tf_idf_vector\")\n",
    "tf_idf_model = idf.fit(df_CleanedDataSparseVector)\n",
    "df_rescaledCleanedData = tf_idf_model.transform(df_CleanedDataSparseVector)\n",
    "df_rescaledCleanedData = df_rescaledCleanedData.select(\"paper_id\", \"tf_idf_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "815a1b2a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(paper_id='498902', tf_idf_vector=SparseVector(1000, {33: 13.8439, 47: 4.596, 79: 4.5494, 97: 4.5325, 138: 4.4866, 170: 26.6474, 354: 4.1522, 368: 4.1302, 394: 4.0846, 482: 7.8051, 491: 3.8889, 541: 3.7983, 550: 3.7834, 566: 3.7571, 581: 3.7152, 596: 3.6986, 622: 3.6484, 632: 3.6256, 663: 3.5658, 670: 3.5395, 720: 6.7871, 723: 3.3875, 762: 3.2929, 764: 3.2917, 773: 3.2717, 797: 3.2131, 820: 3.1488, 826: 3.1306, 837: 6.153, 843: 3.0651, 879: 2.9588, 881: 2.9516, 890: 2.9076, 892: 2.9043, 894: 2.8955, 928: 2.7393, 937: 2.6945, 946: 5.2459, 949: 7.8223, 952: 2.5913, 962: 2.5231, 965: 2.5148, 984: 2.3717, 985: 9.4584, 990: 2.328, 992: 2.3177}))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF Vector for papers\n",
    "df_rescaledCleanedData.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47cf5f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/04 23:01:34 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "21/08/04 23:01:34 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# Latent Direchlet Allocation\n",
    "\n",
    "num_topics = 40\n",
    "\n",
    "# Transform data into LDA supported format\n",
    "df_lda_format = df_CleanedDataSparseVector.selectExpr(\"paper_id AS id\",\"term_frequency_vector AS features\")\n",
    "\n",
    "lda = LDA(k=40)\n",
    "lda_model = lda.fit(df_lda_format)\n",
    "df_lda_paper_topic_model = lda_model.transform(df_lda_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c520fb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|     id|            features|   topicDistribution|\n",
      "+-------+--------------------+--------------------+\n",
      "| 498902|(1000,[33,47,79,9...|[0.03661775127919...|\n",
      "| 201593|(1000,[38,90,104,...|[2.65645710653724...|\n",
      "|1727709|(1000,[24,55,80,1...|[4.49222917643613...|\n",
      "| 383220|(1000,[93,104,114...|[0.05777618935500...|\n",
      "|  23055|(1000,[24,56,225,...|[3.98494353942264...|\n",
      "|1287740|(1000,[79,238,260...|[5.14751023148192...|\n",
      "|2090908|(1000,[177,280,44...|[3.08819633694069...|\n",
      "|9106608|(1000,[8,27,70,89...|[2.05869430618133...|\n",
      "|1857331|(1000,[61,62,169,...|[4.57543715045570...|\n",
      "|2707871|(1000,[0,1,2,4,7,...|[2.08809005148650...|\n",
      "|2798913|(1000,[1,55,56,65...|[3.98494353942264...|\n",
      "| 460407|(1000,[45,101,139...|[2.60052460791407...|\n",
      "| 423550|(1000,[32,54,232,...|[3.92167838156203...|\n",
      "|6500865|(1000,[31,140,194...|[5.37137359436407...|\n",
      "|2945717|(1000,[217,229,24...|[0.05825001748362...|\n",
      "| 674581|(1000,[3,26,154,1...|[4.84464404059742...|\n",
      "| 560037|(1000,[17,84,99,1...|[0.01275281342901...|\n",
      "|1866283|(1000,[134,214,23...|[4.75145624117561...|\n",
      "| 306396|(1000,[89,150,202...|[2.18623762606204...|\n",
      "| 168969|(1000,[0,174,196,...|[3.63326863210611...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LDA Vector for Papers\n",
    "df_lda_paper_topic_model.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "077bb261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#User Profiling\n",
    "\n",
    "#1)  produces a user profile for each user as the summation of the TF-IDF vectors of the papers that appear in the userâ€™s library\n",
    "\n",
    "df_userLibrary_explode = df_userLibrary.selectExpr(\"user_hash_id\",\"explode(user_library) AS paper_id\")\n",
    "df_userJoined_TfIdf = df_userLibrary_explode.join(df_rescaledCleanedData,df_userLibrary_explode.paper_id == df_rescaledCleanedData.paper_id, how=\"inner\").select(df_userLibrary_explode.user_hash_id,df_userLibrary_explode.paper_id,df_rescaledCleanedData.tf_idf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c010e0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "# Adding 2 sparse vectors\n",
    "def addSparseVectors(v1, v2):\n",
    "    values = collections.defaultdict(float) # Initialize Dictionary with default value 0.0\n",
    "    \n",
    "    # Add values from v1 SparseVector\n",
    "    for i in range(v1.indices.size):\n",
    "        values[v1.indices[i]] += v1.values[i]\n",
    "    # Add values from v2 SParseVector\n",
    "    for i in range(v2.indices.size):\n",
    "        values[v2.indices[i]] += v2.values[i]\n",
    "    return Vectors.sparse(v1.size, dict(values))\n",
    "\n",
    "#final Df : summation of TF-IDF vector for each userlibrary\n",
    "df_userprofile_tfidf = df_userJoined_TfIdf.selectExpr(\"user_hash_id\",\"tf_idf_vector\").rdd.reduceByKey(lambda x,y: addSparseVectors(x,y)).toDF().selectExpr(\"_1 AS user_hash_id\",\"_2 AS sum_tf_idf_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25eadd98",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(user_hash_id='6931f7f79678cf72aae416ff7cb43bb1', sum_tf_idf_vector=SparseVector(1000, {0: 13.9931, 1: 23.287, 3: 9.3123, 4: 37.249, 5: 4.6555, 6: 4.6536, 7: 32.5708, 10: 9.2996, 11: 4.6492, 12: 23.2428, 13: 4.6467, 14: 41.8145, 18: 9.2734, 20: 4.633, 22: 4.6305, 23: 41.6692, 24: 4.6299, 26: 97.1893, 27: 9.2561, 28: 37.0147, 29: 4.6256, 31: 4.6238, 33: 4.6146, 34: 13.8384, 35: 4.6122, 36: 13.8293, 37: 23.0429, 38: 13.8257, 39: 9.2147, 41: 4.6044, 42: 9.2087, 47: 18.3839, 48: 9.192, 51: 13.7737, 52: 4.5889, 53: 4.5889, 55: 13.756, 56: 4.583, 57: 9.1601, 60: 13.7331, 62: 13.7243, 65: 4.5707, 67: 9.1298, 68: 4.5643, 70: 13.6809, 72: 173.1171, 74: 95.6219, 75: 36.4183, 76: 9.1034, 77: 4.5511, 78: 4.55, 79: 13.6483, 80: 9.0989, 81: 9.0977, 84: 4.5466, 86: 40.8787, 87: 18.166, 88: 4.5415, 89: 4.5409, 92: 27.2389, 93: 72.5921, 94: 31.7512, 95: 4.5353, 99: 18.1279, 100: 9.0628, 102: 9.0484, 104: 4.5236, 105: 9.045, 107: 9.0362, 110: 4.5153, 111: 13.5444, 113: 4.5148, 115: 9.023, 116: 9.023, 117: 36.0877, 118: 4.5104, 119: 4.5099, 120: 4.5093, 122: 4.5066, 123: 9.0088, 124: 4.5033, 127: 13.5035, 128: 17.9895, 131: 125.806, 132: 8.9861, 133: 13.476, 134: 8.9786, 136: 4.4877, 139: 17.9423, 140: 8.9648, 141: 4.4808, 142: 4.4808, 144: 4.4797, 145: 4.4765, 146: 8.9531, 147: 40.2794, 148: 22.3696, 149: 31.2769, 150: 13.4028, 152: 13.3981, 154: 8.9248, 156: 31.2184, 158: 22.2781, 159: 17.8225, 162: 22.2498, 165: 8.8917, 166: 22.2241, 168: 26.6658, 169: 4.4423, 171: 4.4412, 172: 8.8784, 173: 26.6322, 174: 62.1132, 176: 4.4326, 177: 17.7264, 179: 22.1504, 181: 4.4276, 182: 4.4276, 186: 39.8254, 188: 17.6962, 189: 8.8471, 191: 52.9866, 192: 4.4136, 193: 17.6523, 194: 17.6424, 196: 4.4086, 200: 8.8064, 201: 4.3998, 203: 21.9915, 204: 30.7812, 205: 8.7927, 207: 8.7771, 208: 17.5446, 210: 30.6794, 214: 17.4719, 215: 17.4643, 216: 34.9173, 217: 34.9059, 220: 30.5196, 221: 8.7171, 223: 21.788, 225: 8.7096, 226: 4.3543, 228: 4.3478, 229: 21.7227, 230: 21.7134, 231: 4.3372, 232: 4.3367, 234: 4.3335, 235: 12.9977, 236: 4.3307, 237: 8.6578, 238: 4.3285, 239: 4.3271, 242: 12.9785, 245: 8.6496, 246: 17.2811, 249: 34.5406, 251: 17.2631, 254: 17.2559, 255: 21.5699, 256: 4.3108, 257: 30.1571, 258: 17.229, 259: 4.305, 260: 12.9084, 262: 4.3015, 263: 34.3658, 265: 4.2922, 266: 8.5721, 267: 8.5721, 268: 12.849, 269: 4.2826, 271: 8.5643, 273: 12.8373, 274: 8.5565, 276: 17.1112, 277: 4.2778, 278: 4.2761, 280: 21.3609, 281: 4.2705, 282: 4.27, 283: 38.4226, 284: 4.2687, 285: 12.8037, 287: 17.0493, 289: 21.2903, 290: 8.5153, 291: 12.7564, 292: 4.2471, 293: 101.8495, 295: 80.5752, 297: 12.7199, 299: 4.2362, 300: 8.4708, 301: 12.7062, 304: 16.9151, 306: 21.1335, 307: 16.9019, 310: 4.2193, 311: 8.4362, 313: 16.8675, 314: 8.4224, 315: 8.4183, 316: 21.0417, 317: 12.6141, 319: 12.5949, 320: 16.7916, 321: 4.1943, 322: 8.3878, 323: 12.5769, 327: 41.8676, 328: 4.1868, 329: 29.299, 331: 12.5567, 332: 4.184, 334: 20.9062, 335: 8.3609, 336: 20.9003, 337: 16.7139, 338: 4.1781, 339: 29.2274, 344: 8.3243, 345: 4.161, 346: 66.5696, 347: 4.1594, 348: 20.7953, 351: 12.4737, 352: 4.1552, 353: 4.1541, 354: 33.2172, 356: 8.3013, 358: 16.5934, 360: 20.7284, 361: 157.5214, 362: 4.1453, 363: 20.6962, 366: 8.2642, 369: 4.1302, 371: 4.1206, 374: 4.1136, 375: 8.2271, 376: 37.0187, 377: 24.6637, 378: 8.2176, 379: 28.7564, 380: 20.5366, 381: 8.2066, 383: 8.2008, 385: 32.7945, 386: 24.5915, 387: 8.1928, 388: 8.1914, 390: 4.0917, 391: 28.6421, 392: 8.1713, 394: 40.8456, 395: 8.167, 396: 20.4068, 398: 20.4014, 399: 4.0792, 401: 40.7638, 402: 8.1478, 403: 138.4888, 405: 16.2675, 406: 4.0665, 408: 36.5955, 409: 4.0648, 410: 28.4435, 411: 4.063, 412: 16.22, 414: 4.0488, 416: 32.3793, 420: 40.3782, 421: 4.0375, 422: 36.3128, 423: 8.0634, 424: 12.0931, 426: 56.2882, 427: 12.0597, 428: 76.3657, 430: 52.1592, 431: 4.0106, 433: 4.0083, 434: 12.0099, 435: 116.0866, 436: 32.0186, 437: 4.0, 438: 23.9864, 439: 11.9912, 440: 7.9909, 441: 3.9925, 442: 11.9707, 443: 3.987, 444: 3.9818, 445: 23.887, 446: 11.9329, 448: 15.9041, 449: 7.9514, 450: 7.9482, 451: 23.8427, 452: 11.9175, 455: 35.7039, 456: 3.9655, 460: 31.6609, 463: 11.8531, 465: 15.7668, 466: 3.9389, 467: 3.9374, 468: 11.8112, 469: 7.8692, 470: 7.8649, 471: 11.7863, 472: 7.857, 473: 66.7426, 474: 3.9251, 475: 3.9209, 476: 54.8667, 477: 15.6714, 478: 11.7363, 479: 19.5531, 480: 3.9094, 482: 39.0254, 483: 7.7968, 484: 11.6907, 485: 35.0668, 487: 7.7832, 489: 11.6703, 491: 23.3336, 492: 3.8866, 493: 23.2967, 494: 11.644, 495: 27.1591, 496: 3.8796, 497: 19.3819, 498: 814.0419, 500: 3.8703, 502: 3.8654, 506: 3.86, 507: 57.8913, 508: 212.096, 509: 11.5587, 510: 7.7046, 511: 3.852, 512: 19.2291, 515: 7.6816, 516: 3.8408, 517: 3.8394, 518: 26.86, 520: 7.6726, 521: 7.6715, 522: 11.5064, 523: 7.671, 524: 3.8352, 528: 153.1858, 529: 34.4097, 530: 7.6444, 531: 57.3124, 532: 26.7343, 533: 3.8173, 535: 30.501, 536: 30.4641, 537: 15.2169, 539: 22.8044, 541: 15.1932, 542: 3.7972, 544: 11.3861, 546: 3.7924, 547: 34.1295, 549: 3.7858, 550: 22.7004, 552: 7.5652, 553: 22.6893, 554: 26.4653, 555: 7.5615, 556: 22.6593, 557: 26.423, 558: 7.5447, 559: 30.1476, 560: 124.1704, 561: 3.762, 562: 15.0479, 563: 33.8553, 564: 3.7604, 565: 18.7917, 566: 75.1412, 567: 97.6702, 569: 22.51, 570: 3.7478, 571: 29.9725, 572: 3.7466, 574: 3.7352, 575: 18.6445, 576: 7.4573, 577: 11.1777, 578: 3.7179, 579: 37.1794, 580: 3.716, 581: 48.2979, 582: 3.715, 583: 51.9751, 584: 3.7118, 585: 3.7113, 586: 37.1054, 587: 11.1316, 589: 11.1309, 591: 7.4172, 592: 7.4162, 593: 3.7074, 594: 7.4132, 595: 11.1059, 596: 36.9857, 597: 36.9857, 598: 3.6983, 599: 25.8832, 600: 3.6971, 601: 3.6945, 602: 7.3836, 603: 3.6894, 604: 18.4445, 605: 22.1262, 606: 3.6824, 607: 29.4252, 609: 29.4043, 611: 3.6751, 612: 220.4041, 613: 3.6727, 614: 33.033, 615: 21.9796, 616: 14.6465, 617: 14.6344, 618: 25.6004, 620: 29.2334, 621: 7.3014, 622: 7.2968, 623: 21.8792, 624: 7.2912, 625: 72.9124, 626: 18.2051, 627: 3.6374, 628: 7.2679, 630: 7.2597, 631: 7.2574, 634: 43.4177, 635: 3.6172, 636: 7.2309, 637: 18.0773, 638: 144.5824, 639: 3.6121, 640: 14.4431, 641: 3.6108, 642: 46.845, 644: 28.7802, 646: 7.1889, 647: 28.7277, 648: 7.1736, 649: 43.0341, 650: 3.5816, 651: 32.2327, 652: 14.3213, 653: 46.5107, 654: 35.7732, 655: 50.0734, 657: 3.5743, 658: 14.2947, 659: 32.1457, 661: 7.1388, 662: 42.8173, 663: 17.8289, 664: 3.5658, 665: 7.1269, 666: 7.1218, 667: 21.3476, 669: 10.6624, 670: 7.0791, 671: 3.534, 672: 28.2701, 673: 14.1318, 674: 7.056, 675: 10.5742, 677: 28.1119, 678: 14.0535, 679: 24.5937, 680: 14.0479, 681: 10.522, 682: 3.5029, 683: 45.5305, 684: 10.4987, 685: 24.4955, 686: 27.96, 687: 6.9848, 688: 34.8691, 690: 48.7098, 691: 3.4785, 692: 6.9457, 693: 3.4725, 694: 3.4698, 696: 6.9234, 697: 6.9176, 698: 20.746, 699: 13.8223, 700: 34.4988, 701: 154.9558, 702: 27.5372, 703: 58.5101, 706: 41.2249, 707: 10.3062, 708: 34.3205, 709: 30.8268, 710: 3.423, 711: 20.5015, 712: 6.8298, 713: 10.2425, 714: 10.2381, 715: 64.8312, 716: 17.0317, 717: 6.8087, 718: 13.6102, 719: 44.198, 720: 47.5099, 721: 16.9491, 722: 6.7764, 723: 10.1625, 724: 27.0943, 725: 3.3783, 726: 57.4189, 727: 23.6209, 728: 6.7457, 729: 10.1133, 730: 10.1075, 731: 6.7376, 733: 20.2045, 734: 23.5367, 735: 10.0845, 736: 47.0466, 737: 13.4219, 738: 3.3549, 739: 3.3544, 740: 10.0561, 741: 6.6969, 742: 6.6965, 744: 23.3817, 745: 13.3556, 746: 23.3474, 747: 6.667, 748: 6.6656, 750: 19.9817, 751: 26.6182, 752: 6.6515, 753: 23.2722, 754: 9.9508, 755: 6.6279, 756: 13.2539, 757: 49.697, 758: 19.805, 759: 6.5987, 760: 23.0864, 761: 13.1825, 762: 16.4643, 763: 39.5086, 764: 121.7942, 765: 42.7611, 766: 13.1527, 767: 151.1307, 769: 19.7069, 770: 6.5642, 771: 22.9199, 773: 71.9781, 774: 39.2399, 775: 16.3492, 776: 13.0736, 777: 16.3239, 778: 52.1888, 779: 26.0756, 780: 22.8008, 781: 9.7634, 782: 61.7255, 783: 48.719, 784: 12.9899, 785: 35.6882, 786: 16.2158, 787: 51.7579, 788: 25.8765, 789: 16.1652, 790: 22.6292, 791: 9.6909, 793: 100.1158, 794: 25.8266, 795: 19.3346, 796: 38.597, 797: 41.7706, 799: 19.2716, 800: 12.8287, 801: 6.412, 802: 3.2042, 803: 35.2397, 804: 12.8073, 805: 9.5944, 806: 6.3711, 807: 19.1003, 808: 22.2776, 809: 15.8686, 810: 34.9093, 811: 6.346, 812: 22.208, 813: 3.172, 814: 9.5078, 815: 9.5074, 817: 9.453, 818: 18.8976, 819: 3.1489, 820: 75.57, 821: 9.4421, 822: 28.301, 823: 37.6896, 824: 18.8365, 826: 90.7864, 827: 18.7488, 829: 352.4709, 830: 24.9125, 832: 24.7942, 833: 3.0864, 834: 27.7696, 835: 9.2463, 836: 49.2403, 837: 126.1355, 838: 12.2965, 839: 9.2201, 840: 33.8026, 841: 27.6007, 842: 21.4609, 843: 52.1062, 844: 3.0626, 845: 24.4866, 846: 58.1338, 847: 42.78, 848: 21.3864, 849: 9.145, 850: 12.1731, 851: 6.075, 852: 15.1793, 853: 15.165, 855: 21.2083, 856: 30.2901, 858: 36.3332, 859: 15.1227, 860: 24.1825, 861: 18.1081, 862: 3.0147, 863: 12.0583, 864: 15.0576, 865: 12.0368, 866: 11.9888, 867: 35.9419, 869: 14.9608, 870: 2.9843, 871: 11.9333, 872: 35.7872, 873: 65.5656, 874: 32.7024, 875: 20.7786, 876: 14.8401, 877: 23.7273, 878: 50.3651, 879: 41.4235, 880: 26.5836, 881: 23.6124, 882: 5.8775, 884: 5.8567, 885: 2.9277, 886: 32.1922, 887: 14.5954, 888: 58.2129, 890: 40.7058, 891: 63.901, 892: 46.4682, 894: 23.1642, 895: 57.8714, 896: 22.9445, 897: 45.8704, 898: 22.8762, 901: 8.5206, 902: 5.6796, 903: 8.5135, 905: 8.4532, 906: 16.8158, 907: 72.6684, 908: 55.8909, 910: 30.6914, 911: 8.3648, 912: 44.4816, 913: 38.8672, 914: 49.8924, 915: 55.3842, 916: 19.3784, 917: 8.3033, 918: 8.2904, 919: 146.2777, 920: 13.7865, 921: 22.0569, 922: 11.0197, 923: 283.6513, 924: 22.0063, 925: 16.4974, 926: 10.9922, 927: 2.741, 928: 101.3533, 929: 13.6741, 930: 19.1347, 931: 62.6805, 932: 2.7231, 933: 48.9309, 934: 16.2591, 935: 35.1415, 936: 62.0952, 937: 61.9741, 938: 40.4152, 939: 10.7671, 940: 32.1521, 941: 18.6719, 942: 7.9942, 943: 5.3208, 944: 5.3154, 945: 47.4293, 946: 763.2826, 947: 20.9493, 948: 28.6872, 949: 192.9502, 950: 2.5987, 951: 72.6362, 952: 10.3654, 953: 59.575, 954: 7.7489, 955: 33.534, 956: 59.1278, 957: 74.4137, 958: 7.69, 959: 40.6407, 960: 12.6865, 961: 20.2608, 962: 70.6473, 963: 55.5069, 964: 206.2529, 965: 27.6623, 966: 12.529, 967: 5.0104, 968: 39.958, 969: 9.9796, 970: 9.9019, 971: 68.7158, 972: 63.7748, 973: 14.706, 974: 34.2963, 975: 12.1494, 976: 216.144, 977: 21.8419, 978: 67.8234, 979: 70.1084, 980: 4.8185, 981: 23.9896, 982: 52.3231, 983: 47.4398, 984: 97.241, 985: 243.5546, 986: 23.6071, 987: 7.0669, 988: 30.5977, 989: 95.7551, 990: 13.968, 991: 34.8969, 992: 60.2601, 993: 46.3429, 994: 142.9002, 995: 50.4784, 996: 13.6805, 997: 81.9987, 998: 11.3758, 999: 29.5293}))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_userprofile_tfidf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ad57be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2  LDA-based profiles for each user as the summation of the paper-topics vectors of the papers \n",
    "\n",
    "df_userJoined_LDA = df_userLibrary_explode.join(df_lda_paper_topic_model,df_userLibrary_explode.paper_id == df_lda_paper_topic_model.id, how=\"inner\").select(df_userLibrary_explode.user_hash_id,df_userLibrary_explode.paper_id,df_lda_paper_topic_model.topicDistribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2789094c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#function to add 2 LDA dense vectors\n",
    "def addLDAPaperTopicVectors(v1,v2):\n",
    "    return (v1+v2)\n",
    "    \n",
    "df_userprofile_lda = df_userJoined_LDA.selectExpr(\"user_hash_id\",\"topicDistribution\").rdd.reduceByKey(lambda x,y: addLDAPaperTopicVectors(x,y)).toDF().selectExpr(\"_1 AS user_hash_id\",\"_2 AS sum_lda_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6de48879",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_hash_id='6931f7f79678cf72aae416ff7cb43bb1', sum_lda_vector=DenseVector([0.3177, 2.7137, 0.1215, 0.2662, 0.3415, 0.3474, 0.1214, 0.2329, 0.4597, 1.4139, 0.2767, 0.5888, 0.1218, 18.0877, 0.3621, 0.1308, 0.513, 0.1927, 0.3893, 0.239, 0.9663, 0.5952, 1.1469, 0.3024, 0.1182, 0.1218, 0.122, 0.8721, 0.3173, 3.1643, 0.4668, 0.1197, 0.2334, 0.5449, 0.2284, 0.2457, 1.2261, 15.3653, 4.7057, 9.8997]))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_userprofile_lda.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07fb7118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ex 4.2 Funtion to calculate cosine similarity\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# UDF for calculating cosine similarity\n",
    "\n",
    "#def cos_similarity(a,b):\n",
    "#    cosSimValue = float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "#    return cosSimValue\n",
    "\n",
    "def calculateCosineSimilarity(userVector,paperVector):\n",
    "    cosineSimilarityValue = float(cosine_similarity([userVector],[paperVector])[0,0])\n",
    "    return cosineSimilarityValue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c36475d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ex 4.3 \n",
    "\n",
    "#Broadcast variable containing master set of all the paperId\n",
    "\n",
    "PaperIds = list(df_paperCsv.selectExpr(\"paper_id\").toPandas()['paper_id'])\n",
    "PaperIdsBroadcast = sc.broadcast(PaperIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "370c66a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Find Delta library for the user: Fetching the Papers excluding the papers which are already present in user Library\n",
    "def findDeltaLibrary(userLib):\n",
    "    deltaUserLibrary = np.setdiff1d(PaperIdsBroadcast.value,list(userLib),assume_unique=True).tolist()    \n",
    "    return deltaUserLibrary\n",
    "\n",
    "FindDeltaLibraryUDF = udf(findDeltaLibrary, ArrayType(StringType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9cf4b524",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#a)\n",
    "\n",
    "#Function for Content Based Recommendation on TF-IDF User Profile\n",
    "def tf_idf_CBRS(df_UserRecord,numberOfRecommendation):\n",
    "    \n",
    "\n",
    "    #Fetching the Papers along with their TF-IDF excluding the papers which are already present in user Library\n",
    "    df_UserRecord_DeltaLibrary = df_UserRecord.withColumn('delta_UserLibrary', explode(FindDeltaLibraryUDF(df_UserRecord.user_library)))\n",
    "    df_DeltaPapers = df_UserRecord_DeltaLibrary.join(df_rescaledCleanedData, df_UserRecord_DeltaLibrary.delta_UserLibrary == df_rescaledCleanedData.paper_id,how=\"inner\")\n",
    "    df_DeltaPapers = df_DeltaPapers.selectExpr(\"user_hash_id\",\"sum_tf_idf_vector AS user_tf_idf_vector\",\"paper_id\",\"tf_idf_vector\")\n",
    "    \n",
    "    #Repartioning the data\n",
    "    df_DeltaPapers = df_DeltaPapers.repartition(40)\n",
    "    \n",
    "    #Calculate the Cosine Similarity\n",
    "    df_cosine_similarity_result = df_DeltaPapers.rdd.map(lambda x: (x['user_hash_id'],x['paper_id'],calculateCosineSimilarity(x['user_tf_idf_vector'],x['tf_idf_vector']))).toDF(schema=['user_hash_id', 'paper_id', 'cosine_similarity_value'])\n",
    "    \n",
    "    #Repartioning the data\n",
    "    df_cosine_similarity_result = df_cosine_similarity_result.repartition(40)\n",
    "    \n",
    "    #Fetch the Top K recommendations for the User\n",
    "    windowFn = Window.partitionBy(df_cosine_similarity_result['user_hash_id']).orderBy(df_cosine_similarity_result['cosine_similarity_value'].desc())\n",
    "     \n",
    "    df_top_k_recommendations = df_cosine_similarity_result.select('*', row_number().over(windowFn).alias('row_number')).filter(col('row_number') <= numberOfRecommendation)\n",
    "    df_top_k_recommendations = df_top_k_recommendations.groupBy(\"user_hash_id\").agg(collect_list(\"paper_id\").alias(\"recommended_library\"))\n",
    "    \n",
    "    return df_top_k_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cf6a8639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ex4.3 b)\n",
    "\n",
    "#Function for Content Based Recommendation on LDA User Profile\n",
    "def lda_CBRS(df_UserRecord,numberOfRecommendation):\n",
    "    \n",
    "    #Fetching the Papers along with their LDA excluding the papers which are already present in user Library\n",
    "    df_UserRecord_DeltaLibrary = df_UserRecord.withColumn('delta_UserLibrary', explode(FindDeltaLibraryUDF(df_UserRecord.user_library)))\n",
    "    df_DeltaPapers = df_UserRecord_DeltaLibrary.join(df_lda_paper_topic_model, df_UserRecord_DeltaLibrary.delta_UserLibrary == df_lda_paper_topic_model.id,how=\"inner\")\n",
    "    df_DeltaPapers = df_DeltaPapers.selectExpr(\"user_hash_id\",\"sum_lda_vector AS user_lda_vector\",\"id AS paper_id\",\"topicDistribution\")\n",
    "    \n",
    "    #Repartioning the data\n",
    "    df_DeltaPapers = df_DeltaPapers.repartition(40)\n",
    "    \n",
    "    #Calculate the Cosine Similarity\n",
    "    df_cosine_similarity_result = df_DeltaPapers.rdd.map(lambda x: (x['user_hash_id'],x['paper_id'],calculateCosineSimilarity(x['user_lda_vector'],x['topicDistribution']))).toDF(schema=['user_hash_id', 'paper_id', 'cosine_similarity_value'])\n",
    "    \n",
    "    #Repartioning the data\n",
    "    df_cosine_similarity_result = df_cosine_similarity_result.repartition(40)\n",
    "    \n",
    "    #Fetch the Top K recommendations for the User\n",
    "    windowFn = Window.partitionBy(df_cosine_similarity_result['user_hash_id']).orderBy(df_cosine_similarity_result['cosine_similarity_value'].desc())\n",
    "    \n",
    "    df_top_k_recommendations = df_cosine_similarity_result.select('*', row_number().over(windowFn).alias('row_number')).filter(col('row_number') <= numberOfRecommendation)\n",
    "    df_top_k_recommendations = df_top_k_recommendations.groupBy(\"user_hash_id\").agg(collect_list(\"paper_id\").alias(\"recommended_library\"))\n",
    "    \n",
    "    return df_top_k_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9d9fa44b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Ex 4.3 c) Top K recommendations for User = 1eac022a97d683eace8815545ce3153f\n",
    "\n",
    "user_hash_id = '1eac022a97d683eace8815545ce3153f'\n",
    "k = 10 #No Of Recommendation\n",
    "\n",
    "#Recommendation using TF-IDF Vector\n",
    "\n",
    "#Fetch the records for the Particular User along with User Library\n",
    "df_UserRecord = df_userLibrary.filter((df_userLibrary.user_hash_id.isin(user_hash_id)))\n",
    "    \n",
    "#Fetching the User TF-IDF \n",
    "df_UserRecord = df_UserRecord.join(df_userprofile_tfidf, df_UserRecord.user_hash_id == df_userprofile_tfidf.user_hash_id, how=\"inner\").select(df_UserRecord.user_hash_id,df_userprofile_tfidf.sum_tf_idf_vector,df_UserRecord.user_library)\n",
    "\n",
    "#inputs((userHashId,Vector,UserLibrary),NoOfRecommendation)\n",
    "recommendation_with_tf_idf=tf_idf_CBRS(df_UserRecord,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfde9473",
   "metadata": {},
   "source": [
    "# Top 10 Recommendation for User:1eac022a97d683eace8815545ce3153f with TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d67931fe",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+------------------------------------------------------------------------------------------+\n",
      "|user_hash_id                    |recommended_library                                                                       |\n",
      "+--------------------------------+------------------------------------------------------------------------------------------+\n",
      "|1eac022a97d683eace8815545ce3153f|[854469, 2838456, 4209212, 11692301, 10100718, 6306064, 7326487, 2284574, 3374934, 940716]|\n",
      "+--------------------------------+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Top 10 Recommendation for User:1eac022a97d683eace8815545ce3153f with TF-IDF \n",
    "recommendation_with_tf_idf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1c7baaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Recommendation using LDA vector\n",
    "\n",
    "#Fetch the records for the Particular User along with User Library\n",
    "df_UserRecord = df_userLibrary.filter((df_userLibrary.user_hash_id.isin(user_hash_id)))\n",
    "    \n",
    "#Fetching the User LDA Vector \n",
    "df_UserRecord = df_UserRecord.join(df_userprofile_lda, df_UserRecord.user_hash_id == df_userprofile_lda.user_hash_id, how=\"inner\").select(df_UserRecord.user_hash_id,df_userprofile_lda.sum_lda_vector,df_UserRecord.user_library)\n",
    "\n",
    "#inputs((userHashId,Vector,UserLibrary),NoOfRecommendation)\n",
    "recommendation_with_lda=lda_CBRS(df_UserRecord,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fd411e",
   "metadata": {},
   "source": [
    "# Top 10 Recommendation for User:1eac022a97d683eace8815545ce3153f with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1d6cb1a6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+---------------------------------------------------------------------------------------+\n",
      "|user_hash_id                    |recommended_library                                                                    |\n",
      "+--------------------------------+---------------------------------------------------------------------------------------+\n",
      "|1eac022a97d683eace8815545ce3153f|[361044, 1141545, 697335, 4892261, 2783973, 1522926, 118767, 10314804, 781559, 4285207]|\n",
      "+--------------------------------+---------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Top 10 Recommendation for User:1eac022a97d683eace8815545ce3153f with LDA\n",
    "recommendation_with_lda.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "11127e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3707:(24 + 16) / 40][Stage 3713:> (0 + 0) / 4][Stage 3714:>(0 + 0) / 40]\r"
     ]
    }
   ],
   "source": [
    "#Ex 4.4 Offline evaluation metrics\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "import numpy as np\n",
    "\n",
    "#Find the common elements between two list: to find the Hit elements list\n",
    "def hitElements(list1,list2):\n",
    "    commonElements = np.intersect1d(list1,list2)\n",
    "    return list(commonElements)\n",
    "\n",
    "#df_UserMetricsEvaluation Schema --> user_hash_id, test_set_user_library, top_recommendations\n",
    "def calculatePrecisionAndRecallMetrics(df_UserMetricsEvaluation,k):\n",
    "    \n",
    "    #If list of top-k recommendations is larger than k,then only the first k elements must be taken into account\n",
    "    df_firstKElements = df_UserMetricsEvaluation.withColumn(\"new_top_recommendations\",\n",
    "                                                            when(size(col(\"top_recommendations\")) > k,\n",
    "                                                                 f.slice(\"top_recommendations\",start=1,length=k))\n",
    "                                                            .otherwise(col(\"top_recommendations\")))\n",
    "    \n",
    "    #Find the hitElementsList and Count the Number of Hits\n",
    "    HitElementsUDF = udf(hitElements, ArrayType(StringType()))\n",
    "    \n",
    "    df_HitElements = df_firstKElements.withColumn(\"HitsList\",HitElementsUDF(df_firstKElements.new_top_recommendations,df_firstKElements.test_set_user_library))\\\n",
    "                     .withColumn(\"NoOfHits\",size(col(\"HitsList\")))\n",
    "    \n",
    "    #Calculate the precision for each user\n",
    "    df_precision = df_HitElements.withColumn(\"Precision_Value\",col(\"NoOfHits\")/k)\n",
    "    \n",
    "    #Calculate the recall for each user\n",
    "    df_recall = df_precision.withColumn(\"Recall_Value\",col(\"NoOfHits\")/when(size(col(\"test_set_user_library\")) == 0,1)\n",
    "                                                                        .otherwise(size(col(\"test_set_user_library\"))))\n",
    "    \n",
    "    df_finalMetrics = df_recall.select(\"user_hash_id\",\"test_set_user_library\",\"top_recommendations\",\"new_top_recommendations AS recommendations_considered\",\"NoOfHits\",\"Precision_Value\",\"Recall_Value\")\n",
    "  \n",
    "    return df_finalMetrics\n",
    "\n",
    "\n",
    "#Calculate the MRR value\n",
    "def calculateMRR(recommendationList,HitList):\n",
    "    \n",
    "    mrrvalue = 0\n",
    "    \n",
    "    if(len(HitList) == 0):\n",
    "        mrrvalue = 0    \n",
    "    else:\n",
    "        firstElement = HitList[0]\n",
    "        position = recommendationList.index(firstElement)\n",
    "        mrrvalue = position + 1\n",
    "        \n",
    "    return mrrvalue\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "28abed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ex 3.5 \n",
    "# 1: Randomly selects n users\n",
    "fraction_of_users = 20/(df_userLibrary.count())\n",
    "df_sampleUsers = df_userLibrary.sample(withReplacement=False, fraction=fraction_of_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "eb4af81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ex3.5\n",
    "#2 Divide sampled data into raining set and test set for each user\n",
    "import math\n",
    "import random\n",
    "\n",
    "def divideList(masterList,trainingSetFraction):\n",
    "    size_masterList = len(masterList)\n",
    "    size_TrainingList = int(math.ceil(trainingSetFraction * size_masterList))\n",
    "    trainingList = masterList[:size_TrainingList]\n",
    "    testList = masterList[size_TrainingList:]\n",
    "    return [trainingList,testList]\n",
    "\n",
    "df_divide_train_test_data = df_sampleUsers.rdd.map(lambda x: (x[0],divideList(x[1],0.8))).toDF()\n",
    "df_divide_train_test_data = df_divide_train_test_data.select(df_divide_train_test_data._1.alias(\"user_hash_id\"),df_divide_train_test_data._2[0].alias(\"training_data\"),df_divide_train_test_data._2[1].alias(\"test_data\"))\n",
    "\n",
    "df_training_data = df_divide_train_test_data.selectExpr(\"user_hash_id\",\"training_data\")\n",
    "df_test_data = df_divide_train_test_data.selectExpr(\"user_hash_id\",\"test_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cada1ebe",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|        user_hash_id|       training_data|\n",
      "+--------------------+--------------------+\n",
      "|488fb15e8c77f8054...|[1523301, 5281566...|\n",
      "|39dc28d81c6f4d6e6...|[130743, 130683, ...|\n",
      "|c4328f856a7c676ff...|[257231, 225187, ...|\n",
      "|eebe1b4e5663deee3...|[9077591, 774343,...|\n",
      "|fb65d75b2051b110a...|[685336, 556249, ...|\n",
      "|610d9dcb3071cf289...|[1276165, 3835963...|\n",
      "|3f6288d7b3e733e18...|[2602918, 2941832...|\n",
      "|26b1c5cbd64854ffc...|[1041371, 261696,...|\n",
      "|d8224267d1409bbcf...|  [7659888, 8951552]|\n",
      "|1b0f9f2a96f40cef5...|[963530, 780088, ...|\n",
      "|1f9cb9801e3f9372b...|[6527806, 261292,...|\n",
      "|be833c5784aed1c75...|[13510470, 895012...|\n",
      "|cf6f250eac956fe6f...|           [1226827]|\n",
      "|9ed3bf65a07ac3007...|[553783, 515205, ...|\n",
      "|27994f3f86658586a...|[543594, 531781, ...|\n",
      "|e5e6be374e14a8252...|[5999093, 1254633...|\n",
      "|655b753a309dbe4dc...|[941245, 316837, ...|\n",
      "|73b3c17934cf71742...|[6041009, 8185963...|\n",
      "|9b20a80145e4c8c3d...|[144027, 4165285,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_training_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c7546c6b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|        user_hash_id|           test_data|\n",
      "+--------------------+--------------------+\n",
      "|488fb15e8c77f8054...|            [352724]|\n",
      "|39dc28d81c6f4d6e6...|  [3190640, 1037773]|\n",
      "|c4328f856a7c676ff...|    [220081, 208311]|\n",
      "|eebe1b4e5663deee3...|                  []|\n",
      "|fb65d75b2051b110a...|[197329, 556247, ...|\n",
      "|610d9dcb3071cf289...|           [3718551]|\n",
      "|3f6288d7b3e733e18...|            [691779]|\n",
      "|26b1c5cbd64854ffc...|           [2283761]|\n",
      "|d8224267d1409bbcf...|                  []|\n",
      "|1b0f9f2a96f40cef5...|    [967051, 102131]|\n",
      "|1f9cb9801e3f9372b...|[261290, 1058754,...|\n",
      "|be833c5784aed1c75...|                  []|\n",
      "|cf6f250eac956fe6f...|                  []|\n",
      "|9ed3bf65a07ac3007...|[3914589, 6593246...|\n",
      "|27994f3f86658586a...|[540907, 613662, ...|\n",
      "|e5e6be374e14a8252...|[6609221, 1347966...|\n",
      "|655b753a309dbe4dc...|            [975289]|\n",
      "|73b3c17934cf71742...|[6350815, 3456526...|\n",
      "|9b20a80145e4c8c3d...|[305879, 5826636,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5e0adb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Ex 3.5\n",
    "\n",
    "#Creating User profiles for Training data set\n",
    "df_training_data_explode = df_training_data.selectExpr(\"user_hash_id\",\"explode(training_data) AS training_data_paper_id\")\n",
    "\n",
    "#a- user profile using sampled users data over Tf-Idf vector\n",
    "\n",
    "df_training_data_TfIdf = df_training_data_explode.join(df_rescaledCleanedData,df_training_data_explode.training_data_paper_id == df_rescaledCleanedData.paper_id, how=\"inner\").select(df_training_data_explode.user_hash_id,df_training_data_explode.training_data_paper_id,df_rescaledCleanedData.tf_idf_vector)\n",
    "df_training_data_userprofile_tfidf = df_training_data_TfIdf.selectExpr(\"user_hash_id\",\"tf_idf_vector\").rdd.reduceByKey(lambda x,y: addSparseVectors(x,y)).toDF().selectExpr(\"_1 AS user_hash_id\",\"_2 AS sum_tf_idf_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d470e0bc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_hash_id='1b0f9f2a96f40cef5384836ee74bf7b9', sum_tf_idf_vector=SparseVector(1000, {25: 18.5147, 32: 4.6189, 49: 4.5936, 67: 4.5649, 78: 4.55, 164: 4.4469, 188: 4.424, 191: 4.4156, 233: 4.3358, 237: 4.3289, 259: 4.305, 261: 4.3024, 269: 29.9781, 284: 4.2687, 297: 4.24, 312: 4.2177, 318: 29.4049, 321: 4.1943, 333: 4.1836, 335: 4.1804, 342: 4.1633, 352: 20.7761, 356: 4.1506, 372: 4.1161, 380: 4.1073, 386: 4.0986, 389: 8.1849, 433: 4.0083, 437: 4.0, 443: 3.987, 449: 7.9514, 457: 15.857, 493: 7.7656, 504: 3.864, 520: 3.8363, 523: 7.671, 524: 7.6704, 547: 3.7922, 555: 3.7808, 559: 3.7685, 562: 3.762, 569: 3.7517, 574: 3.7352, 577: 3.7259, 587: 3.7105, 593: 3.7074, 598: 7.3967, 612: 7.3468, 622: 3.6484, 633: 14.5013, 636: 3.6155, 643: 7.1968, 651: 3.5814, 652: 10.741, 654: 10.732, 656: 14.2972, 657: 3.5743, 661: 7.1388, 677: 3.514, 679: 3.5134, 682: 28.0236, 685: 3.4994, 700: 3.4499, 703: 3.4418, 705: 3.4393, 706: 3.4354, 711: 6.8338, 713: 6.8283, 715: 3.4122, 716: 3.4063, 717: 3.4044, 725: 3.3783, 731: 3.3688, 734: 3.3624, 736: 6.7209, 737: 3.3555, 742: 10.0448, 743: 3.3477, 747: 6.667, 758: 9.9025, 760: 3.2981, 766: 3.2882, 770: 3.2821, 780: 6.5145, 782: 9.7461, 791: 12.9212, 794: 6.4567, 801: 9.6179, 804: 3.2018, 805: 3.1981, 806: 3.1856, 808: 3.1825, 817: 6.302, 818: 3.1496, 820: 3.1488, 825: 15.6866, 829: 6.2384, 830: 18.6843, 832: 3.0993, 839: 6.1467, 840: 3.073, 842: 3.0658, 847: 3.0557, 850: 3.0433, 851: 3.0375, 852: 3.0359, 856: 3.029, 860: 3.0228, 864: 3.0115, 883: 11.7278, 884: 8.7851, 885: 2.9277, 888: 2.9106, 893: 8.6876, 899: 8.5754, 906: 5.6053, 908: 2.7945, 909: 2.7905, 910: 2.7901, 912: 2.7801, 915: 2.7692, 925: 2.7496, 927: 10.9642, 932: 10.8925, 935: 5.4064, 938: 2.6943, 944: 5.3154, 945: 2.635, 948: 10.4317, 954: 2.583, 956: 5.1415, 957: 2.566, 961: 2.5326, 966: 2.5058, 969: 2.4949, 970: 2.4755, 973: 4.902, 975: 2.4299, 977: 4.8538, 978: 4.8445, 981: 4.7979, 983: 2.372, 984: 4.7435, 987: 2.3556, 988: 2.3537, 990: 18.624, 996: 2.2801, 998: 29.5771}))]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training_data_userprofile_tfidf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "fd9dd41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#b)- User profile using sampled users data over LDA vector\n",
    "\n",
    "df_training_data_LDA = df_training_data_explode.join(df_lda_paper_topic_model,df_training_data_explode.training_data_paper_id == df_lda_paper_topic_model.id, how=\"inner\").select(df_training_data_explode.user_hash_id,df_training_data_explode.training_data_paper_id,df_lda_paper_topic_model.topicDistribution)\n",
    "    \n",
    "df_training_data_userprofile_lda = df_training_data_LDA.selectExpr(\"user_hash_id\",\"topicDistribution\").rdd.reduceByKey(lambda x,y: addLDAPaperTopicVectors(x,y)).toDF().selectExpr(\"_1 AS user_hash_id\",\"_2 AS sum_lda_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "05af705b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_hash_id='1b0f9f2a96f40cef5384836ee74bf7b9', sum_lda_vector=DenseVector([1.5708, 0.0401, 0.3668, 3.8009, 0.4158, 0.0404, 0.041, 0.04, 0.0405, 0.1624, 0.0404, 0.0402, 0.0412, 0.0418, 1.0043, 0.1534, 0.3845, 0.0407, 0.3614, 0.0408, 0.0402, 0.0401, 0.0405, 0.0407, 0.0399, 0.4334, 0.8312, 0.5911, 0.0404, 0.1925, 0.0409, 0.3165, 0.4003, 0.0404, 0.0409, 0.0406, 0.0404, 0.0407, 0.0408, 0.0412]))]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training_data_userprofile_lda.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "40ce0be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ex4.5 Off-line evaluation\n",
    "\n",
    "##########  LDA  ########\n",
    "\n",
    "#Caluclating recommendation for Sampled users training data\n",
    "\n",
    "#Making the dataframe format compatible with CBRS function\n",
    "df_UserRecord = df_training_data_userprofile_lda.join(df_training_data,df_training_data_userprofile_lda.user_hash_id == df_training_data.user_hash_id , how=\"inner\")\\\n",
    "                .select(df_training_data.user_hash_id,df_training_data_userprofile_lda.sum_lda_vector,df_training_data.training_data.alias(\"user_library\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "65c75706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                0]\r"
     ]
    }
   ],
   "source": [
    "#fetching top 50 recommendations\n",
    "recommendation_with_lda=lda_CBRS(df_UserRecord,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5e7119bc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|        user_hash_id| recommended_library|\n",
      "+--------------------+--------------------+\n",
      "|c4328f856a7c676ff...|[10722036, 917926...|\n",
      "|488fb15e8c77f8054...|[955708, 2645008,...|\n",
      "|39dc28d81c6f4d6e6...|[1429875, 845770,...|\n",
      "|73b3c17934cf71742...|[854077, 13113325...|\n",
      "|27994f3f86658586a...|[708229, 2710660,...|\n",
      "|610d9dcb3071cf289...|[4058315, 1259066...|\n",
      "|655b753a309dbe4dc...|[1014193, 1979432...|\n",
      "|cf6f250eac956fe6f...|[10522254, 658448...|\n",
      "|e5e6be374e14a8252...|[5969584, 2880302...|\n",
      "|1b0f9f2a96f40cef5...|[2645008, 787476,...|\n",
      "|26b1c5cbd64854ffc...|[3197951, 567792,...|\n",
      "|fb65d75b2051b110a...|[469589, 401403, ...|\n",
      "|9ed3bf65a07ac3007...|[562067, 1188019,...|\n",
      "|eebe1b4e5663deee3...|[6286182, 2945990...|\n",
      "|3f6288d7b3e733e18...|[3454708, 340017,...|\n",
      "|d8224267d1409bbcf...|[1053002, 5806383...|\n",
      "|1f9cb9801e3f9372b...|[1169705, 2392088...|\n",
      "|be833c5784aed1c75...|[3843892, 2226990...|\n",
      "|9b20a80145e4c8c3d...|[1340708, 5996736...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recommendation_with_lda.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "23cd7097",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formating dataframe to the format accepted by function CalculatePreceisonAndRecall\n",
    "df_lda_evaluationMetrics = recommendation_with_lda.join(df_test_data,recommendation_with_lda.user_hash_id==df_test_data.user_hash_id,how=\"inner\")\\\n",
    ".select(df_test_data.user_hash_id,df_test_data.test_data.alias(\"test_set_user_library\"),recommendation_with_lda.recommended_library.alias(\"top_recommendations\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "97c0700f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3540:=====================================================>(74 + 1) / 75]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+--------------------+--------+---------------+------------+\n",
      "|        user_hash_id|test_set_user_library| top_recommendations|NoOfHits|Precision_Value|Recall_Value|\n",
      "+--------------------+---------------------+--------------------+--------+---------------+------------+\n",
      "|6ce6873d8f198e310...| [1096185, 4371317...|[2908449, 4028922...|       0|            0.0|         0.0|\n",
      "|e49265f34f9326761...|                   []|[9866432, 2306778...|       0|            0.0|         0.0|\n",
      "|b117b747c51aeab14...|    [6080985, 698109]|[4977, 2399933, 2...|       0|            0.0|         0.0|\n",
      "|ea2e1d4f415235bf5...|   [2562165, 1714788]|[7055148, 6776666...|       0|            0.0|         0.0|\n",
      "|cc00a76d4e82fcc19...| [635565, 635575, ...|[2819775, 2753445...|       0|            0.0|         0.0|\n",
      "|44f5e138abead2399...| [893420, 708641, ...|[364201, 10595765...|       0|            0.0|         0.0|\n",
      "|aa7828a01be2c0d45...|                   []|[2828338, 669566,...|       0|            0.0|         0.0|\n",
      "|99cbd4d275cba9cd6...| [4186264, 5955156...|[805377, 1525266,...|       0|            0.0|         0.0|\n",
      "|0f6caef2112e234ec...|                   []|[1748322, 3992954...|       0|            0.0|         0.0|\n",
      "|396a4b54e25ab431a...|                   []|[6243830, 3063532...|       0|            0.0|         0.0|\n",
      "|7a0508ff7b6d660c8...|   [6755656, 4742465]|[10715071, 593052...|       0|            0.0|         0.0|\n",
      "|45eccdf5a4b044b9a...|            [1450024]|[143130, 2547568,...|       0|            0.0|         0.0|\n",
      "|c3c43ac375f1ed3e5...|                   []|[3630171, 3631358...|       0|            0.0|         0.0|\n",
      "|8e1ef679975da5399...| [2919556, 2889947...|[4410010, 1012642...|       0|            0.0|         0.0|\n",
      "|bd5c513e46456a9e1...| [5149340, 803131,...|[1934642, 7800021...|       0|            0.0|         0.0|\n",
      "|b6233c0c7d522ea55...| [1278871, 347188,...|[5687617, 9860709...|       0|            0.0|         0.0|\n",
      "|3cae46f778208a446...| [1450735, 423826,...|[1532710, 246407,...|       0|            0.0|         0.0|\n",
      "|b9553017ff941247f...|                   []|[11067843, 895138...|       0|            0.0|         0.0|\n",
      "|0cd6bb856a6ab2c99...|                   []|[6588803, 6809795...|       0|            0.0|         0.0|\n",
      "+--------------------+---------------------+--------------------+--------+---------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#When K=5\n",
    "df_lda_evaluation_Kis5 = calculatePrecisionAndRecallMetrics(df_lda_evaluationMetrics,5)\n",
    "df_lda_evaluation_Kis5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "5bad8c66",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3610:=====================================================>(74 + 1) / 75]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+--------------------+--------+---------------+------------+\n",
      "|        user_hash_id|test_set_user_library| top_recommendations|NoOfHits|Precision_Value|Recall_Value|\n",
      "+--------------------+---------------------+--------------------+--------+---------------+------------+\n",
      "|6ce6873d8f198e310...| [1096185, 4371317...|[2908449, 4028922...|       0|            0.0|         0.0|\n",
      "|e49265f34f9326761...|                   []|[9866432, 2306778...|       0|            0.0|         0.0|\n",
      "|b117b747c51aeab14...|    [6080985, 698109]|[4977, 2399933, 2...|       0|            0.0|         0.0|\n",
      "|ea2e1d4f415235bf5...|   [2562165, 1714788]|[7055148, 6776666...|       0|            0.0|         0.0|\n",
      "|cc00a76d4e82fcc19...| [635565, 635575, ...|[2819775, 2753445...|       0|            0.0|         0.0|\n",
      "|44f5e138abead2399...| [893420, 708641, ...|[364201, 10595765...|       0|            0.0|         0.0|\n",
      "|aa7828a01be2c0d45...|                   []|[2828338, 669566,...|       0|            0.0|         0.0|\n",
      "|99cbd4d275cba9cd6...| [4186264, 5955156...|[805377, 1525266,...|       0|            0.0|         0.0|\n",
      "|0f6caef2112e234ec...|                   []|[1748322, 3992954...|       0|            0.0|         0.0|\n",
      "|396a4b54e25ab431a...|                   []|[6243830, 3063532...|       0|            0.0|         0.0|\n",
      "|7a0508ff7b6d660c8...|   [6755656, 4742465]|[10715071, 593052...|       0|            0.0|         0.0|\n",
      "|45eccdf5a4b044b9a...|            [1450024]|[143130, 2547568,...|       0|            0.0|         0.0|\n",
      "|c3c43ac375f1ed3e5...|                   []|[3630171, 3631358...|       0|            0.0|         0.0|\n",
      "|8e1ef679975da5399...| [2919556, 2889947...|[4410010, 1012642...|       0|            0.0|         0.0|\n",
      "|bd5c513e46456a9e1...| [5149340, 803131,...|[1934642, 7800021...|       0|            0.0|         0.0|\n",
      "|b6233c0c7d522ea55...| [1278871, 347188,...|[5687617, 9860709...|       0|            0.0|         0.0|\n",
      "|3cae46f778208a446...| [1450735, 423826,...|[1532710, 246407,...|       0|            0.0|         0.0|\n",
      "|b9553017ff941247f...|                   []|[11067843, 895138...|       0|            0.0|         0.0|\n",
      "|0cd6bb856a6ab2c99...|                   []|[6588803, 6809795...|       0|            0.0|         0.0|\n",
      "+--------------------+---------------------+--------------------+--------+---------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#When K=10\n",
    "df_lda_evaluation_Kis10 = calculatePrecisionAndRecallMetrics(df_lda_evaluationMetrics,10)\n",
    "df_lda_evaluation_Kis10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "426fd950",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/06 10:11:47 ERROR Executor: Exception in task 54.0 in stage 3680.0 (TID 37093)\n",
      "net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)\n",
      "\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n",
      "\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/08/06 10:11:47 WARN TaskSetManager: Lost task 54.0 in stage 3680.0 (TID 37093) (d451ade18897 executor driver): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)\n",
      "\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n",
      "\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "21/08/06 10:11:47 ERROR TaskSetManager: Task 54 in stage 3680.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o4316.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 54 in stage 3680.0 failed 1 times, most recent failure: Lost task 54.0 in stage 3680.0 (TID 37093) (d451ade18897 executor driver): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat jdk.internal.reflect.GeneratedMethodAccessor231.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24579/2716182067.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#When K=30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_lda_evaluation_Kis30\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculatePrecisionAndRecallMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_lda_evaluationMetrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_lda_evaluation_Kis30\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4316.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 54 in stage 3680.0 failed 1 times, most recent failure: Lost task 54.0 in stage 3680.0 (TID 37093) (d451ade18897 executor driver): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat jdk.internal.reflect.GeneratedMethodAccessor231.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/06 10:11:48 WARN TaskSetManager: Lost task 36.0 in stage 3680.0 (TID 37091) (d451ade18897 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "#When K=30\n",
    "df_lda_evaluation_Kis30 = calculatePrecisionAndRecallMetrics(df_lda_evaluationMetrics,30)\n",
    "df_lda_evaluation_Kis30.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "7ad52075",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ex4.5 Off-line evaluation\n",
    "\n",
    "##########  TF-IDF  ########\n",
    "\n",
    "#Caluclating recommendation for Sampled users training data\n",
    "\n",
    "#Making the dataframe format compatible with CBRS function\n",
    "df_UserRecord = df_training_data_userprofile_tfidf.join(df_training_data,df_training_data_userprofile_tfidf.user_hash_id == df_training_data.user_hash_id , how=\"inner\")\\\n",
    "                .select(df_training_data.user_hash_id,df_training_data_userprofile_tfidf.sum_tf_idf_vector,df_training_data.training_data.alias(\"user_library\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "e16824a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    }
   ],
   "source": [
    "#fetching top 50 recommendations\n",
    "recommendation_with_tf_idf=tf_idf_CBRS(df_UserRecord,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "5369b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formating dataframe to the format accepted by function CalculatePreceisonAndRecall\n",
    "df_tfidf_evaluationMetrics = recommendation_with_tf_idf.join(df_test_data,recommendation_with_tf_idf.user_hash_id==df_test_data.user_hash_id,how=\"inner\")\\\n",
    ".select(df_test_data.user_hash_id,df_test_data.test_data.alias(\"test_set_user_library\"),recommendation_with_tf_idf.recommended_library.alias(\"top_recommendations\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "7fcfbe38",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3707:(8 + 16) / 40][Stage 3713:> (0 + 0) / 4][Stage 3714:>(0 + 0) / 40]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24579/3542727953.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#When K=5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_tfidf_evaluation_Kis5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculatePrecisionAndRecallMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_tfidf_evaluationMetrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_tfidf_evaluation_Kis5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#When K=5\n",
    "df_tfidf_evaluation_Kis5 = calculatePrecisionAndRecallMetrics(df_tfidf_evaluationMetrics,5)\n",
    "df_tfidf_evaluation_Kis5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4583ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When K=10\n",
    "df_tfidf_evaluation_Kis10 = calculatePrecisionAndRecallMetrics(df_tfidf_evaluationMetrics,10)\n",
    "df_tfidf_evaluation_Kis10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389cdb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When K=30\n",
    "df_tfidf_evaluation_Kis30 = calculatePrecisionAndRecallMetrics(df_tfidf_evaluationMetrics,30)\n",
    "df_tfidf_evaluation_Kis30.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "94996978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3707:(24 + 16) / 40][Stage 3713:> (0 + 0) / 4][Stage 3714:>(0 + 0) / 40]\r"
     ]
    }
   ],
   "source": [
    "len(list([1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cca7450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
