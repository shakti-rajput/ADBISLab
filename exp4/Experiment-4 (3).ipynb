{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc2f8297",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all required packages\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SparkSession,SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import StopWordsRemover, RegexTokenizer,HashingTF\n",
    "from pyspark.ml import Pipeline\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.mllib.clustering import LDA\n",
    "from pyspark.ml.feature import IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe8470e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/07/23 14:24:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Initialise Spark Session\n",
    "spark = SparkSession.builder.appName(\"Experiment4\").config(\"spark.sql.broadcastTimeout\", \"36000\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90a85afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPARING DATAFRAMES FOR DATSETS\n",
    "\n",
    "#Authors Dataframe\n",
    "#df_authors = spark.read.csv(\"Datasets/authors.csv\", sep = \",\", header = True, quote = '\"')\n",
    "\n",
    "#PaperCsv dataframe\n",
    "papersCsvSchema = StructType([\n",
    "    StructField(\"paper_id\",StringType(),False),\n",
    "    StructField(\"type\",StringType(),False),\n",
    "    StructField(\"journal\",StringType(),False),\n",
    "    StructField(\"book_title\",StringType(),False),\n",
    "    StructField(\"series\",StringType(),False),\n",
    "    StructField(\"publisher\",StringType(),False),\n",
    "    StructField(\"pages\",StringType(),False),\n",
    "    StructField(\"volume\",StringType(),False),\n",
    "    StructField(\"number\",StringType(),False),\n",
    "    StructField(\"year\",StringType(),False),\n",
    "    StructField(\"month\",StringType(),False),\n",
    "    StructField(\"postedat\",StringType(),False),\n",
    "    StructField(\"address\",StringType(),False),\n",
    "    StructField(\"title\",StringType(),False),\n",
    "    StructField(\"abstract\",StringType(),False),\n",
    "])\n",
    "df_paperCsv = spark.read.csv(\"Datasets/papers.csv\", sep = \",\", header = False, schema = papersCsvSchema, quote = '\"')\n",
    "\n",
    "#UserLibrary dataframe\n",
    "userLibrarySchema = StructType([\n",
    "    StructField(\"user_hash_id\",StringType(),False),\n",
    "    StructField(\"user_library\",StringType(),False)\n",
    "])\n",
    "df_userLibrary = spark.read.csv(\"Datasets/users_libraries.txt\", sep = \";\", header = False, schema = userLibrarySchema)\n",
    "df_userLibrary = df_userLibrary.selectExpr(\"user_hash_id\",\"split(user_library,',') AS user_library\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d72406e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Cleaning and Tokenizing the data\n",
    "def phraseTokenization(x):\n",
    "    rawPhrase = x[13] + \" \" + x[14] #concatenating title and abstract\n",
    "    rawPhrase = rawPhrase.replace(\"-\",\"\") #removing - from phrase\n",
    "    rawPhrase = rawPhrase.replace(\"_\",\"\") #removing _ from phrase\n",
    "    rawPhrase = rawPhrase.strip() #removing any trailing or leading whitespaces\n",
    "    \n",
    "    #spliting phrase based on non-alphaNumeric characters\n",
    "    phraseArray = re.split('[^a-zA-Z0-9]+',rawPhrase) \n",
    "    \n",
    "    #remove words with less than 3 char\n",
    "    phraseArrayFilteredWords = [i for i in phraseArray if len(i) >= 3]\n",
    "    \n",
    "    return (x[0],list(phraseArrayFilteredWords))\n",
    "\n",
    "\n",
    "df_tokenize = df_paperCsv.na.fill(value=\"\").rdd.map(phraseTokenization).toDF()\n",
    "\n",
    "#Removing StopWords using ML\n",
    "swRemover = StopWordsRemover(inputCol=\"_2\", outputCol=\"cleaned_terms\")\n",
    "df_cleanedData = swRemover.transform(df_tokenize)\n",
    "df_cleanedData = df_cleanedData.selectExpr(\"_1 AS paper_id\",\"cleaned_terms\")\n",
    "\n",
    "#Stemming using Porter stemmer Algo\n",
    "ps =  PorterStemmer()\n",
    "\n",
    "def stemmingTerms(x):\n",
    "    stemmedWords = []\n",
    "    for word in x:\n",
    "        rootWord = ps.stem(word)\n",
    "        stemmedWords.append(rootWord)\n",
    "    return stemmedWords\n",
    "\n",
    "df_cleanedData = df_cleanedData.rdd.mapValues(stemmingTerms).toDF().selectExpr(\"_1 AS paper_id\",\"_2 AS cleaned_terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75cdb7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Find the count of papers in which the term is present\n",
    "df_paperCount = df_cleanedData.selectExpr(\"paper_id\",\"explode(cleaned_terms) AS terms\").distinct().groupBy(\"terms\").count().withColumnRenamed(\"count\", \"paper_count\")\n",
    "\n",
    "#10 percent of total papers present in file\n",
    "noOfDistinctPapers_df = int(df_cleanedData.select(countDistinct(\"paper_id\")).collect()[0][0])\n",
    "tenPercentOfTotalPapers = int(noOfDistinctPapers_df/10)\n",
    "\n",
    "# remove words appear in more than 10% of the papers and keep only the words that appear in at least 20 papers \n",
    "df_filterdTerms = df_paperCount.filter((df_paperCount[\"paper_count\"]<=tenPercentOfTotalPapers) & (df_paperCount[\"paper_count\"]>=20))\n",
    "\n",
    "#Fetch top 1000 terms \n",
    "top1000Terms = df_filterdTerms.orderBy(col(\"paper_count\").desc()).limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "945bfff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/07/23 14:25:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "21/07/23 14:26:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "21/07/23 14:27:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "21/07/23 14:27:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "21/07/23 14:27:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "21/07/23 14:27:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "21/07/23 14:27:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "21/07/23 14:27:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "21/07/23 14:27:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "21/07/23 14:27:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "21/07/23 14:27:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "21/07/23 14:27:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "21/07/23 14:27:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#associate unique integer values to each term\n",
    "df_termsWithUniqueIndex = top1000Terms.withColumn(\"unique_index\",row_number().over(Window.orderBy(\"paper_count\"))).selectExpr(\"terms\",\"unique_index-1 AS unique_index\")\n",
    "\n",
    "#Collect all terms in a list\n",
    "terms_collection =  [row.terms for row in df_termsWithUniqueIndex.collect()]\n",
    "\n",
    "# Generating Termfrequency Vector for each paper\n",
    "df_cleanedDataExplode = df_cleanedData.selectExpr(\"paper_id\",\"explode(cleaned_terms) AS terms\")\n",
    "\n",
    "#Getting the Unique_index of term \n",
    "df_cleanedDataJoinIndex = df_cleanedDataExplode.join(df_termsWithUniqueIndex,df_cleanedDataExplode.terms == df_termsWithUniqueIndex.terms , how = \"inner\").select(df_cleanedDataExplode.paper_id,df_cleanedDataExplode.terms,df_termsWithUniqueIndex.unique_index)\n",
    "\n",
    "#Getting the term_frequency in each paper\n",
    "df_cleanedDataJoinIndex = df_cleanedDataJoinIndex.groupBy(\"paper_id\",\"unique_index\").count().withColumnRenamed(\"count\", \"term_frquency\")\n",
    "\n",
    "#Creating a sparseVector respresentation for each paper\n",
    "rdd_CleanedDataReducedByPaperId = df_cleanedDataJoinIndex.rdd.map(lambda x: (x[0], [(x[1], x[2])])).reduceByKey(lambda a, b: a + b)\n",
    "rdd_CleanedDataReducedByPaperId = rdd_CleanedDataReducedByPaperId.map(lambda x: (x[0],Vectors.sparse(1000,x[1])))\n",
    "\n",
    "df_CleanedDataSparseVector = rdd_CleanedDataReducedByPaperId.toDF().selectExpr(\"_1 AS paper_id\",\"_2 AS term_frequency_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ede0195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(paper_id='498902', term_frequency_vector=SparseVector(1000, {33: 3.0, 47: 1.0, 79: 1.0, 97: 1.0, 138: 1.0, 170: 6.0, 354: 1.0, 368: 1.0, 394: 1.0, 482: 2.0, 491: 1.0, 541: 1.0, 550: 1.0, 566: 1.0, 581: 1.0, 596: 1.0, 622: 1.0, 632: 1.0, 663: 1.0, 670: 1.0, 720: 2.0, 723: 1.0, 762: 1.0, 764: 1.0, 773: 1.0, 797: 1.0, 820: 1.0, 826: 1.0, 837: 2.0, 843: 1.0, 879: 1.0, 881: 1.0, 890: 1.0, 892: 1.0, 894: 1.0, 928: 1.0, 937: 1.0, 946: 2.0, 949: 3.0, 952: 1.0, 962: 1.0, 965: 1.0, 984: 1.0, 985: 4.0, 990: 1.0, 992: 1.0}))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_CleanedDataSparseVector.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "838aab53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Ex 3.2\n",
    "\n",
    "#TF-IDF Representation for each paper\n",
    "\n",
    "idf = IDF(inputCol=\"term_frequency_vector\", outputCol=\"tf_idf_vector\")\n",
    "tf_idf_model = idf.fit(df_CleanedDataSparseVector)\n",
    "df_rescaledCleanedData = tf_idf_model.transform(df_CleanedDataSparseVector)\n",
    "df_rescaledCleanedData = df_rescaledCleanedData.select(\"paper_id\", \"tf_idf_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "815a1b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(paper_id='498902', tf_idf_vector=SparseVector(1000, {33: 13.8439, 47: 4.596, 79: 4.5494, 97: 4.5325, 138: 4.4866, 170: 26.6474, 354: 4.1522, 368: 4.1302, 394: 4.0846, 482: 7.8051, 491: 3.8889, 541: 3.7983, 550: 3.7834, 566: 3.7571, 581: 3.7152, 596: 3.6986, 622: 3.6484, 632: 3.6256, 663: 3.5658, 670: 3.5395, 720: 6.7871, 723: 3.3875, 762: 3.2929, 764: 3.2917, 773: 3.2717, 797: 3.2131, 820: 3.1488, 826: 3.1306, 837: 6.153, 843: 3.0651, 879: 2.9588, 881: 2.9516, 890: 2.9076, 892: 2.9043, 894: 2.8955, 928: 2.7393, 937: 2.6945, 946: 5.2459, 949: 7.8223, 952: 2.5913, 962: 2.5231, 965: 2.5148, 984: 2.3717, 985: 9.4584, 990: 2.328, 992: 2.3177}))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF Vector for papers\n",
    "df_rescaledCleanedData.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47cf5f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/07/23 14:27:54 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "21/07/23 14:27:54 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# Latent Direchlet Allocation\n",
    "\n",
    "num_topics = 40\n",
    "\n",
    "# Transform data into LDA supported format\n",
    "df_lda_format = df_CleanedDataSparseVector.selectExpr(\"paper_id AS id\",\"term_frequency_vector AS features\")\n",
    "\n",
    "lda = LDA(k=40)\n",
    "lda_model = lda.fit(df_lda_format)\n",
    "df_lda_paper_topic_model = lda_model.transform(df_lda_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c520fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+\n",
      "|      id|            features|   topicDistribution|\n",
      "+--------+--------------------+--------------------+\n",
      "|  498902|(1000,[33,47,79,9...|[4.03650087749653...|\n",
      "| 1287740|(1000,[79,238,260...|[5.29822334292367...|\n",
      "| 1727709|(1000,[24,55,80,1...|[4.62375661378785...|\n",
      "| 1857331|(1000,[61,62,169,...|[0.22429080108466...|\n",
      "|  201593|(1000,[38,90,104,...|[2.73423551399914...|\n",
      "| 2090908|(1000,[177,280,44...|[0.23517238339902...|\n",
      "|   23055|(1000,[24,56,225,...|[4.10161835414327...|\n",
      "|  383220|(1000,[93,104,114...|[5.65153260076279...|\n",
      "| 9106608|(1000,[8,27,70,89...|[2.11897089729369...|\n",
      "|  460407|(1000,[45,101,139...|[2.67666537728423...|\n",
      "| 2707871|(1000,[0,1,2,4,7,...|[2.14922757491267...|\n",
      "| 2798913|(1000,[1,55,56,65...|[4.10161835414327...|\n",
      "|  423550|(1000,[32,54,232,...|[4.03650087749653...|\n",
      "| 6500865|(1000,[31,140,194...|[5.52864108584840...|\n",
      "| 2945717|(1000,[217,229,24...|[0.08378920976069...|\n",
      "|  674581|(1000,[3,26,154,1...|[4.9864896784521E...|\n",
      "|  560037|(1000,[17,84,99,1...|[1.55993735223124...|\n",
      "|10101645|(1000,[244,246,29...|[4.70940079911409...|\n",
      "|10886724|(1000,[66,96,147,...|[4.89057347583635...|\n",
      "|  168969|(1000,[0,174,196,...|[3.73964687158730...|\n",
      "+--------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LDA Vector for Papers\n",
    "df_lda_paper_topic_model.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "077bb261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#User Profiling\n",
    "\n",
    "#1)  produces a user profile for each user as the summation of the TF-IDF vectors of the papers that appear in the user’s library\n",
    "\n",
    "df_userLibrary_explode = df_userLibrary.selectExpr(\"user_hash_id\",\"explode(user_library) AS paper_id\")\n",
    "df_userJoined_TfIdf = df_userLibrary_explode.join(df_rescaledCleanedData,df_userLibrary_explode.paper_id == df_rescaledCleanedData.paper_id, how=\"inner\").select(df_userLibrary_explode.user_hash_id,df_userLibrary_explode.paper_id,df_rescaledCleanedData.tf_idf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c010e0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "# Adding 2 sparse vectors\n",
    "def addSparseVectors(v1, v2):\n",
    "    values = collections.defaultdict(float) # Initialize Dictionary with default value 0.0\n",
    "    \n",
    "    # Add values from v1 SparseVector\n",
    "    for i in range(v1.indices.size):\n",
    "        values[v1.indices[i]] += v1.values[i]\n",
    "    # Add values from v2 SParseVector\n",
    "    for i in range(v2.indices.size):\n",
    "        values[v2.indices[i]] += v2.values[i]\n",
    "    return Vectors.sparse(v1.size, dict(values))\n",
    "\n",
    "#final Df : summation of TF-IDF vector for each userlibrary\n",
    "df_userprofile_tfidf = df_userJoined_TfIdf.selectExpr(\"user_hash_id\",\"tf_idf_vector\").rdd.reduceByKey(lambda x,y: addSparseVectors(x,y)).toDF().selectExpr(\"_1 AS user_hash_id\",\"_2 AS sum_tf_idf_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25eadd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(user_hash_id='6931f7f79678cf72aae416ff7cb43bb1', sum_tf_idf_vector=SparseVector(1000, {0: 13.9931, 1: 23.287, 3: 9.3123, 4: 37.249, 5: 4.6555, 6: 4.6536, 7: 32.5708, 10: 9.2996, 11: 4.6492, 12: 23.2428, 13: 4.6467, 14: 41.8145, 18: 9.2734, 20: 4.633, 22: 4.6305, 23: 41.6692, 24: 4.6299, 26: 97.1893, 27: 9.2561, 28: 37.0147, 29: 4.6256, 31: 4.6238, 33: 4.6146, 34: 13.8384, 35: 4.6122, 36: 13.8293, 37: 23.0429, 38: 13.8257, 39: 9.2147, 41: 4.6044, 42: 9.2087, 47: 18.3839, 48: 9.192, 51: 13.7737, 52: 4.5889, 53: 4.5889, 55: 13.756, 56: 4.583, 57: 9.1601, 60: 13.7331, 62: 13.7243, 65: 4.5707, 67: 9.1298, 68: 4.5643, 70: 13.6809, 72: 173.1171, 74: 95.6219, 75: 36.4183, 76: 9.1034, 77: 4.5511, 78: 4.55, 79: 13.6483, 80: 9.0989, 81: 9.0977, 84: 4.5466, 86: 40.8787, 87: 18.166, 88: 4.5415, 89: 4.5409, 92: 27.2389, 93: 72.5921, 94: 31.7512, 95: 4.5353, 99: 18.1279, 100: 9.0628, 102: 9.0484, 104: 4.5236, 105: 9.045, 107: 9.0362, 110: 4.5153, 111: 13.5444, 113: 4.5148, 115: 9.023, 116: 9.023, 117: 36.0877, 118: 4.5104, 119: 4.5099, 120: 4.5093, 122: 4.5066, 123: 9.0088, 124: 4.5033, 127: 13.5035, 128: 17.9895, 131: 125.806, 132: 8.9861, 133: 13.476, 134: 8.9786, 136: 4.4877, 139: 17.9423, 140: 8.9648, 141: 4.4808, 142: 4.4808, 144: 4.4797, 145: 4.4765, 146: 8.9531, 147: 40.2794, 148: 22.3696, 149: 31.2769, 150: 13.4028, 152: 13.3981, 154: 8.9248, 156: 31.2184, 158: 22.2781, 159: 17.8225, 162: 22.2498, 165: 8.8917, 166: 22.2241, 168: 26.6658, 169: 4.4423, 171: 4.4412, 172: 8.8784, 173: 26.6322, 174: 62.1132, 176: 4.4326, 177: 17.7264, 179: 22.1504, 181: 4.4276, 182: 4.4276, 186: 39.8254, 188: 17.6962, 189: 8.8471, 191: 52.9866, 192: 4.4136, 193: 17.6523, 194: 17.6424, 196: 4.4086, 200: 8.8064, 201: 4.3998, 203: 21.9915, 204: 30.7812, 205: 8.7927, 207: 8.7771, 208: 17.5446, 210: 30.6794, 214: 17.4719, 215: 17.4643, 216: 34.9173, 217: 34.9059, 220: 30.5196, 221: 8.7171, 223: 21.788, 225: 8.7096, 226: 4.3543, 228: 4.3478, 229: 21.7227, 230: 21.7134, 231: 4.3372, 232: 4.3367, 234: 4.3335, 235: 12.9977, 236: 4.3307, 237: 8.6578, 238: 4.3285, 239: 4.3271, 242: 12.9785, 245: 8.6496, 246: 17.2811, 249: 34.5406, 251: 17.2631, 254: 17.2559, 255: 21.5699, 256: 4.3108, 257: 30.1571, 258: 17.229, 259: 4.305, 260: 12.9084, 262: 4.3015, 263: 34.3658, 265: 4.2922, 266: 8.5721, 267: 8.5721, 268: 12.849, 269: 4.2826, 271: 8.5643, 273: 12.8373, 274: 8.5565, 276: 17.1112, 277: 4.2778, 278: 4.2761, 280: 21.3609, 281: 4.2705, 282: 4.27, 283: 38.4226, 284: 4.2687, 285: 12.8037, 287: 17.0493, 289: 21.2903, 290: 8.5153, 291: 12.7564, 292: 4.2471, 293: 101.8495, 295: 80.5752, 297: 12.7199, 299: 4.2362, 300: 8.4708, 301: 12.7062, 304: 16.9151, 306: 21.1335, 307: 16.9019, 310: 4.2193, 311: 8.4362, 313: 16.8675, 314: 8.4224, 315: 8.4183, 316: 21.0417, 317: 12.6141, 319: 12.5949, 320: 16.7916, 321: 4.1943, 322: 8.3878, 323: 12.5769, 327: 41.8676, 328: 4.1868, 329: 29.299, 331: 12.5567, 332: 4.184, 334: 20.9062, 335: 8.3609, 336: 20.9003, 337: 16.7139, 338: 4.1781, 339: 29.2274, 344: 8.3243, 345: 4.161, 346: 66.5696, 347: 4.1594, 348: 20.7953, 351: 12.4737, 352: 4.1552, 353: 4.1541, 354: 33.2172, 356: 8.3013, 358: 16.5934, 360: 20.7284, 361: 157.5214, 362: 4.1453, 363: 20.6962, 366: 8.2642, 369: 4.1302, 371: 4.1206, 374: 4.1136, 375: 8.2271, 376: 37.0187, 377: 24.6637, 378: 8.2176, 379: 28.7564, 380: 20.5366, 381: 8.2066, 383: 8.2008, 385: 32.7945, 386: 24.5915, 387: 8.1928, 388: 8.1914, 390: 4.0917, 391: 28.6421, 392: 8.1713, 394: 40.8456, 395: 8.167, 396: 20.4068, 398: 20.4014, 399: 4.0792, 401: 40.7638, 402: 8.1478, 403: 138.4888, 405: 16.2675, 406: 4.0665, 408: 36.5955, 409: 4.0648, 410: 28.4435, 411: 4.063, 412: 16.22, 414: 4.0488, 416: 32.3793, 420: 40.3782, 421: 4.0375, 422: 36.3128, 423: 8.0634, 424: 12.0931, 426: 56.2882, 427: 12.0597, 428: 76.3657, 430: 52.1592, 431: 4.0106, 433: 4.0083, 434: 12.0099, 435: 116.0866, 436: 32.0186, 437: 4.0, 438: 23.9864, 439: 11.9912, 440: 7.9909, 441: 3.9925, 442: 11.9707, 443: 3.987, 444: 3.9818, 445: 23.887, 446: 11.9329, 448: 15.9041, 449: 7.9514, 450: 7.9482, 451: 23.8427, 452: 11.9175, 455: 35.7039, 456: 3.9655, 460: 31.6609, 463: 11.8531, 465: 15.7668, 466: 3.9389, 467: 3.9374, 468: 11.8112, 469: 7.8692, 470: 7.8649, 471: 11.7863, 472: 7.857, 473: 66.7426, 474: 3.9251, 475: 3.9209, 476: 54.8667, 477: 15.6714, 478: 11.7363, 479: 19.5531, 480: 3.9094, 482: 39.0254, 483: 7.7968, 484: 11.6907, 485: 35.0668, 487: 7.7832, 489: 11.6703, 491: 23.3336, 492: 3.8866, 493: 23.2967, 494: 11.644, 495: 27.1591, 496: 3.8796, 497: 19.3819, 498: 814.0419, 500: 3.8703, 502: 3.8654, 506: 3.86, 507: 57.8913, 508: 212.096, 509: 11.5587, 510: 7.7046, 511: 3.852, 512: 19.2291, 515: 7.6816, 516: 3.8408, 517: 3.8394, 518: 26.86, 520: 7.6726, 521: 7.6715, 522: 11.5064, 523: 7.671, 524: 3.8352, 528: 153.1858, 529: 34.4097, 530: 7.6444, 531: 57.3124, 532: 26.7343, 533: 3.8173, 535: 30.501, 536: 30.4641, 537: 15.2169, 539: 22.8044, 541: 15.1932, 542: 3.7972, 544: 11.3861, 546: 3.7924, 547: 34.1295, 549: 3.7858, 550: 22.7004, 552: 7.5652, 553: 22.6893, 554: 26.4653, 555: 7.5615, 556: 22.6593, 557: 26.423, 558: 7.5447, 559: 30.1476, 560: 124.1704, 561: 3.762, 562: 15.0479, 563: 33.8553, 564: 3.7604, 565: 18.7917, 566: 75.1412, 567: 97.6702, 569: 22.51, 570: 3.7478, 571: 29.9725, 572: 3.7466, 574: 3.7352, 575: 18.6445, 576: 7.4573, 577: 11.1777, 578: 3.7179, 579: 37.1794, 580: 3.716, 581: 48.2979, 582: 3.715, 583: 51.9751, 584: 3.7118, 585: 3.7113, 586: 37.1054, 587: 11.1316, 589: 11.1309, 591: 7.4172, 592: 7.4162, 593: 3.7074, 594: 7.4132, 595: 11.1059, 596: 36.9857, 597: 36.9857, 598: 3.6983, 599: 25.8832, 600: 3.6971, 601: 3.6945, 602: 7.3836, 603: 3.6894, 604: 18.4445, 605: 22.1262, 606: 3.6824, 607: 29.4252, 609: 29.4043, 611: 3.6751, 612: 220.4041, 613: 3.6727, 614: 33.033, 615: 21.9796, 616: 14.6465, 617: 14.6344, 618: 25.6004, 620: 29.2334, 621: 7.3014, 622: 7.2968, 623: 21.8792, 624: 7.2912, 625: 72.9124, 626: 18.2051, 627: 3.6374, 628: 7.2679, 630: 7.2597, 631: 7.2574, 634: 43.4177, 635: 3.6172, 636: 7.2309, 637: 18.0773, 638: 144.5824, 639: 3.6121, 640: 14.4431, 641: 3.6108, 642: 46.845, 644: 28.7802, 646: 7.1889, 647: 28.7277, 648: 7.1736, 649: 43.0341, 650: 3.5816, 651: 32.2327, 652: 14.3213, 653: 46.5107, 654: 35.7732, 655: 50.0734, 657: 3.5743, 658: 14.2947, 659: 32.1457, 661: 7.1388, 662: 42.8173, 663: 17.8289, 664: 3.5658, 665: 7.1269, 666: 7.1218, 667: 21.3476, 669: 10.6624, 670: 7.0791, 671: 3.534, 672: 28.2701, 673: 14.1318, 674: 7.056, 675: 10.5742, 677: 28.1119, 678: 14.0535, 679: 24.5937, 680: 14.0479, 681: 10.522, 682: 3.5029, 683: 45.5305, 684: 10.4987, 685: 24.4955, 686: 27.96, 687: 6.9848, 688: 34.8691, 690: 48.7098, 691: 3.4785, 692: 6.9457, 693: 3.4725, 694: 3.4698, 696: 6.9234, 697: 6.9176, 698: 20.746, 699: 13.8223, 700: 34.4988, 701: 154.9558, 702: 27.5372, 703: 58.5101, 706: 41.2249, 707: 10.3062, 708: 34.3205, 709: 30.8268, 710: 3.423, 711: 20.5015, 712: 6.8298, 713: 10.2425, 714: 10.2381, 715: 64.8312, 716: 17.0317, 717: 6.8087, 718: 13.6102, 719: 44.198, 720: 47.5099, 721: 16.9491, 722: 6.7764, 723: 10.1625, 724: 27.0943, 725: 3.3783, 726: 57.4189, 727: 23.6209, 728: 6.7457, 729: 10.1133, 730: 10.1075, 731: 6.7376, 733: 20.2045, 734: 23.5367, 735: 10.0845, 736: 47.0466, 737: 13.4219, 738: 3.3549, 739: 3.3544, 740: 10.0561, 741: 6.6969, 742: 6.6965, 744: 23.3817, 745: 13.3556, 746: 23.3474, 747: 6.667, 748: 6.6656, 750: 19.9817, 751: 26.6182, 752: 6.6515, 753: 23.2722, 754: 9.9508, 755: 6.6279, 756: 13.2539, 757: 49.697, 758: 19.805, 759: 6.5987, 760: 23.0864, 761: 13.1825, 762: 16.4643, 763: 39.5086, 764: 121.7942, 765: 42.7611, 766: 13.1527, 767: 151.1307, 769: 19.7069, 770: 6.5642, 771: 22.9199, 773: 71.9781, 774: 39.2399, 775: 16.3492, 776: 13.0736, 777: 16.3239, 778: 52.1888, 779: 26.0756, 780: 22.8008, 781: 9.7634, 782: 61.7255, 783: 48.719, 784: 12.9899, 785: 35.6882, 786: 16.2158, 787: 51.7579, 788: 25.8765, 789: 16.1652, 790: 22.6292, 791: 9.6909, 793: 100.1158, 794: 25.8266, 795: 19.3346, 796: 38.597, 797: 41.7706, 799: 19.2716, 800: 12.8287, 801: 6.412, 802: 3.2042, 803: 35.2397, 804: 12.8073, 805: 9.5944, 806: 6.3711, 807: 19.1003, 808: 22.2776, 809: 15.8686, 810: 34.9093, 811: 6.346, 812: 22.208, 813: 3.172, 814: 9.5078, 815: 9.5074, 817: 9.453, 818: 18.8976, 819: 3.1489, 820: 75.57, 821: 9.4421, 822: 28.301, 823: 37.6896, 824: 18.8365, 826: 90.7864, 827: 18.7488, 829: 352.4709, 830: 24.9125, 832: 24.7942, 833: 3.0864, 834: 27.7696, 835: 9.2463, 836: 49.2403, 837: 126.1355, 838: 12.2965, 839: 9.2201, 840: 33.8026, 841: 27.6007, 842: 21.4609, 843: 52.1062, 844: 3.0626, 845: 24.4866, 846: 58.1338, 847: 42.78, 848: 21.3864, 849: 9.145, 850: 12.1731, 851: 6.075, 852: 15.1793, 853: 15.165, 855: 21.2083, 856: 30.2901, 858: 36.3332, 859: 15.1227, 860: 24.1825, 861: 18.1081, 862: 3.0147, 863: 12.0583, 864: 15.0576, 865: 12.0368, 866: 11.9888, 867: 35.9419, 869: 14.9608, 870: 2.9843, 871: 11.9333, 872: 35.7872, 873: 65.5656, 874: 32.7024, 875: 20.7786, 876: 14.8401, 877: 23.7273, 878: 50.3651, 879: 41.4235, 880: 26.5836, 881: 23.6124, 882: 5.8775, 884: 5.8567, 885: 2.9277, 886: 32.1922, 887: 14.5954, 888: 58.2129, 890: 40.7058, 891: 63.901, 892: 46.4682, 894: 23.1642, 895: 57.8714, 896: 22.9445, 897: 45.8704, 898: 22.8762, 901: 8.5206, 902: 5.6796, 903: 8.5135, 905: 8.4532, 906: 16.8158, 907: 72.6684, 908: 55.8909, 910: 30.6914, 911: 8.3648, 912: 44.4816, 913: 38.8672, 914: 49.8924, 915: 55.3842, 916: 19.3784, 917: 8.3033, 918: 8.2904, 919: 146.2777, 920: 13.7865, 921: 22.0569, 922: 11.0197, 923: 283.6513, 924: 22.0063, 925: 16.4974, 926: 10.9922, 927: 2.741, 928: 101.3533, 929: 13.6741, 930: 19.1347, 931: 62.6805, 932: 2.7231, 933: 48.9309, 934: 16.2591, 935: 35.1415, 936: 62.0952, 937: 61.9741, 938: 40.4152, 939: 10.7671, 940: 32.1521, 941: 18.6719, 942: 7.9942, 943: 5.3208, 944: 5.3154, 945: 47.4293, 946: 763.2826, 947: 20.9493, 948: 28.6872, 949: 192.9502, 950: 2.5987, 951: 72.6362, 952: 10.3654, 953: 59.575, 954: 7.7489, 955: 33.534, 956: 59.1278, 957: 74.4137, 958: 7.69, 959: 40.6407, 960: 12.6865, 961: 20.2608, 962: 70.6473, 963: 55.5069, 964: 206.2529, 965: 27.6623, 966: 12.529, 967: 5.0104, 968: 39.958, 969: 9.9796, 970: 9.9019, 971: 68.7158, 972: 63.7748, 973: 14.706, 974: 34.2963, 975: 12.1494, 976: 216.144, 977: 21.8419, 978: 67.8234, 979: 70.1084, 980: 4.8185, 981: 23.9896, 982: 52.3231, 983: 47.4398, 984: 97.241, 985: 243.5546, 986: 23.6071, 987: 7.0669, 988: 30.5977, 989: 95.7551, 990: 13.968, 991: 34.8969, 992: 60.2601, 993: 46.3429, 994: 142.9002, 995: 50.4784, 996: 13.6805, 997: 81.9987, 998: 11.3758, 999: 29.5293}))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_userprofile_tfidf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ad57be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2  LDA-based profiles for each user as the summation of the paper-topics vectors of the papers \n",
    "\n",
    "df_userJoined_LDA = df_userLibrary_explode.join(df_lda_paper_topic_model,df_userLibrary_explode.paper_id == df_lda_paper_topic_model.id, how=\"inner\").select(df_userLibrary_explode.user_hash_id,df_userLibrary_explode.paper_id,df_lda_paper_topic_model.topicDistribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2789094c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#function to add 2 LDA dense vectors\n",
    "def addLDAPaperTopicVectors(v1,v2):\n",
    "    return (v1+v2)\n",
    "    \n",
    "df_userprofile_lda = df_userJoined_LDA.selectExpr(\"user_hash_id\",\"topicDistribution\").rdd.reduceByKey(lambda x,y: addLDAPaperTopicVectors(x,y)).toDF().selectExpr(\"_1 AS user_hash_id\",\"_2 AS sum_lda_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6de48879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_hash_id='6931f7f79678cf72aae416ff7cb43bb1', sum_lda_vector=DenseVector([0.1754, 1.0947, 0.1784, 0.3062, 0.1359, 3.9344, 0.4657, 0.1238, 0.5484, 0.3686, 0.4487, 0.4528, 0.6498, 0.1356, 0.464, 0.2493, 0.7315, 0.3416, 0.381, 1.4015, 0.3333, 0.9788, 0.4322, 0.1196, 0.1184, 16.3355, 0.5066, 0.929, 0.9196, 0.5298, 0.9098, 6.4886, 0.1819, 0.933, 0.119, 0.1212, 9.7864, 0.3584, 15.1234, 0.1883]))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_userprofile_lda.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28abed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ex 3.5 \n",
    "# 1: Randomly selects n users\n",
    "fraction_of_users = 0.5\n",
    "df_sampleUsers = df_userLibrary.sample(withReplacement=False, fraction=fraction_of_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb4af81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ex3.5\n",
    "#2 Divide sampled data into raining set and test set for each user\n",
    "import math\n",
    "import random\n",
    "\n",
    "def divideList(masterList,trainingSetFraction):\n",
    "    size_masterList = len(masterList)\n",
    "    size_TrainingList = int(math.ceil(trainingSetFraction * size_masterList))\n",
    "    trainingList = masterList[:size_TrainingList]\n",
    "    testList = masterList[size_TrainingList:]\n",
    "    return [trainingList,testList]\n",
    "\n",
    "df_divide_train_test_data = df_sampleUsers.rdd.map(lambda x: (x[0],divideList(x[1],0.8))).toDF()\n",
    "df_divide_train_test_data = df_divide_train_test_data.select(df_divide_train_test_data._1.alias(\"user_hash_id\"),df_divide_train_test_data._2[0].alias(\"training_data\"),df_divide_train_test_data._2[1].alias(\"test_data\"))\n",
    "\n",
    "df_training_data = df_divide_train_test_data.selectExpr(\"user_hash_id\",\"training_data\")\n",
    "df_test_data = df_divide_train_test_data.selectExpr(\"user_hash_id\",\"test_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cada1ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|        user_hash_id|       training_data|\n",
      "+--------------------+--------------------+\n",
      "|f05bcffe7951de9e5...|[1158654, 478707,...|\n",
      "|ca4f1ba4094011d9a...|            [278019]|\n",
      "|d1d41a15201915503...|[6610569, 6493797...|\n",
      "|b656009a6efdc8b1a...|[771870, 181369, ...|\n",
      "|cf9c7f356092c34be...|             [90558]|\n",
      "|d85f7d83f27b3f533...|[7610843, 3633347...|\n",
      "|10fdfaf945d5c27ad...|           [2010550]|\n",
      "|7e070a9da96672e05...|           [1071959]|\n",
      "|3b715ebaf1f8f81a1...|[4119394, 3378798...|\n",
      "|488fb15e8c77f8054...|[1523301, 5281566...|\n",
      "|c6b59086a0bbac141...|[2230995, 3050075...|\n",
      "|0ad6516296d95068c...|[2734645, 1218426...|\n",
      "|f3c28e50db4ce8ad8...|[2856540, 2994495...|\n",
      "|38fe6373389d12b5b...|[7276116, 255799,...|\n",
      "|60a321bf89d186c88...|   [1391145, 467189]|\n",
      "|b36c3189bb1457cd0...|[2270229, 6417010...|\n",
      "|7c3219ff9046172ce...|[1089965, 1089982...|\n",
      "|7c0081293b3988065...|[3453059, 3007833...|\n",
      "|4c8912d1b04471cf5...|[3579579, 1931121...|\n",
      "|1291485dbe1a86857...|[4521968, 4240809...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_training_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7546c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|        user_hash_id|           test_data|\n",
      "+--------------------+--------------------+\n",
      "|f05bcffe7951de9e5...|[1453872, 671699,...|\n",
      "|ca4f1ba4094011d9a...|                  []|\n",
      "|d1d41a15201915503...|[7465494, 7329626...|\n",
      "|b656009a6efdc8b1a...|[1129585, 404679,...|\n",
      "|cf9c7f356092c34be...|                  []|\n",
      "|d85f7d83f27b3f533...|[3585, 1458475, 1...|\n",
      "|10fdfaf945d5c27ad...|                  []|\n",
      "|7e070a9da96672e05...|                  []|\n",
      "|3b715ebaf1f8f81a1...|[3399487, 3150261...|\n",
      "|488fb15e8c77f8054...|            [352724]|\n",
      "|c6b59086a0bbac141...|  [1429875, 2406965]|\n",
      "|0ad6516296d95068c...|   [179911, 1127804]|\n",
      "|f3c28e50db4ce8ad8...|[2881619, 2913739...|\n",
      "|38fe6373389d12b5b...|[10064582, 740518...|\n",
      "|60a321bf89d186c88...|                  []|\n",
      "|b36c3189bb1457cd0...|               [151]|\n",
      "|7c3219ff9046172ce...|[2301235, 1089946...|\n",
      "|7c0081293b3988065...|[1274492, 2765646...|\n",
      "|4c8912d1b04471cf5...|[2050637, 100088,...|\n",
      "|1291485dbe1a86857...|  [1442986, 9174409]|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 643, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "df_test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e0adb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Ex 3.5\n",
    "\n",
    "#Creating User profiles for Training data set\n",
    "df_training_data_explode = df_training_data.selectExpr(\"user_hash_id\",\"explode(training_data) AS training_data_paper_id\")\n",
    "\n",
    "#a- user profile using sampled users data over Tf-Idf vector\n",
    "\n",
    "df_training_data_TfIdf = df_training_data_explode.join(df_rescaledCleanedData,df_training_data_explode.training_data_paper_id == df_rescaledCleanedData.paper_id, how=\"inner\").select(df_training_data_explode.user_hash_id,df_training_data_explode.training_data_paper_id,df_rescaledCleanedData.tf_idf_vector)\n",
    "df_training_data_userprofile_tfidf = df_training_data_TfIdf.selectExpr(\"user_hash_id\",\"tf_idf_vector\").rdd.reduceByKey(lambda x,y: addSparseVectors(x,y)).toDF().selectExpr(\"_1 AS user_hash_id\",\"_2 AS sum_tf_idf_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd9dd41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#b)- User profile using sampled users data over LDA vector\n",
    "\n",
    "df_training_data_LDA = df_training_data_explode.join(df_lda_paper_topic_model,df_training_data_explode.training_data_paper_id == df_lda_paper_topic_model.id, how=\"inner\").select(df_training_data_explode.user_hash_id,df_training_data_explode.training_data_paper_id,df_lda_paper_topic_model.topicDistribution)\n",
    "def addLDAPaperTopicVectors(v1,v2):\n",
    "    return (v1+v2)\n",
    "    \n",
    "df_training_data_userprofile_lda = df_training_data_LDA.selectExpr(\"user_hash_id\",\"topicDistribution\").rdd.reduceByKey(lambda x,y: addLDAPaperTopicVectors(x,y)).toDF().selectExpr(\"_1 AS user_hash_id\",\"_2 AS sum_lda_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05af705b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_hash_id='f0fe2969ce44deace9595fabd7c12bdf', sum_lda_vector=DenseVector([23.6102, 1.6307, 12.4924, 11.9339, 2.5502, 3.2708, 40.4139, 11.0825, 8.5738, 5.9339, 7.6117, 1.6016, 2.4992, 6.6816, 25.7266, 8.4814, 14.6735, 7.1139, 6.0141, 5.8428, 14.739, 4.3305, 37.0701, 5.0813, 5.5494, 1.3507, 6.554, 4.8049, 4.6017, 4.5837, 11.1863, 4.1229, 4.1246, 7.6146, 5.3945, 3.5363, 1.3626, 5.0872, 1.393, 4.7739]))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training_data_userprofile_lda.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07fb7118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ex 4.2 Funtion to calculate cosine similarity\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# UDF for calculating cosine similarity\n",
    "\n",
    "#def cos_similarity(a,b):\n",
    "#    cosSimValue = float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "#    return cosSimValue\n",
    "\n",
    "def calculateCosineSimilarity(userVector,paperVector):\n",
    "    cosineSimilarityValue = float(cosine_similarity([userVector],[paperVector])[0,0])\n",
    "    return cosineSimilarityValue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9cf4b524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n"
     ]
    }
   ],
   "source": [
    "#Ex 4.3 \n",
    "#a)\n",
    "\n",
    "#Function for Content Based Recommendation on TF-IDF User Profile\n",
    "def tf_idf_CBRS(userId,numberOfRecommendation):\n",
    "    \n",
    "    #Fetch the records for the Particular User along with User Library\n",
    "    df_UserRecord = df_userLibrary.filter(df_userLibrary.user_hash_id == userId)\n",
    "    \n",
    "    #Fetching the User TF-IDF \n",
    "    df_UserRecord = df_UserRecord.join(df_userprofile_tfidf, df_UserRecord.user_hash_id == df_userprofile_tfidf.user_hash_id, how=\"inner\").select(df_UserRecord.user_hash_id,df_userprofile_tfidf.sum_tf_idf_vector,df_UserRecord.user_library)\n",
    "    \n",
    "    #Fetching the Papers along with their TF-IDF excluding the papers which are already present in user Library\n",
    "    list_userPaperLibrary = list(df_UserRecord.collect()[0][2])\n",
    "    df_DeltaPapers = df_rescaledCleanedData.filter(~df_rescaledCleanedData.paper_id.isin(list_userPaperLibrary))\n",
    "    \n",
    "    #Repartioning the data\n",
    "    df_UserRecord = df_UserRecord.repartition(40)\n",
    "    df_DeltaPapers = df_DeltaPapers.repartition(40)\n",
    "    \n",
    "    #Cross Join User with each paper\n",
    "    df_User_XJoin_Papers = df_UserRecord.selectExpr(\"user_hash_id\",\"sum_tf_idf_vector AS user_tf_idf_vector\").crossJoin(df_DeltaPapers)\n",
    "    \n",
    "    #Calculate the Cosine Similarity\n",
    "    df_cosine_similarity_result = df_User_XJoin_Papers.rdd.map(lambda x: (x[0],x[2],calculateCosineSimilarity(x[1],x[3]))).toDF(schema=['user_hash_id', 'paper_id', 'cosine_similarity_value'])\n",
    "    \n",
    "    #Fetch the Top K recommendations for the User\n",
    "    df_top_k_recommendations = df_cosine_similarity_result.orderBy(col(\"cosine_similarity_value\").desc()).limit(numberOfRecommendation)\n",
    "    df_top_k_recommendations = df_top_k_recommendations.groupBy(\"user_hash_id\").agg(collect_list(\"paper_id\").alias(\"recommended_library\"),collect_list(\"cosine_similarity_value\").alias(\"cosine_similarity_recommended_library\"))\n",
    "    \n",
    "    return df_top_k_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf6a8639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ex4.3 b)\n",
    "\n",
    "#Function for Content Based Recommendation on LDA User Profile\n",
    "def lda_CBRS(userId,numberOfRecommendation):\n",
    "    \n",
    "    #Fetch the records for the Particular User along with User Library\n",
    "    df_UserRecord = df_userLibrary.filter(df_userLibrary.user_hash_id == userId)\n",
    "    \n",
    "    #Fetching the User LDA Vector \n",
    "    df_UserRecord = df_UserRecord.join(df_userprofile_lda, df_UserRecord.user_hash_id == df_userprofile_lda.user_hash_id, how=\"inner\").select(df_UserRecord.user_hash_id,df_userprofile_lda.sum_lda_vector,df_UserRecord.user_library)\n",
    "    \n",
    "    #Fetching the Papers along with their LDA excluding the papers which are already present in user Library\n",
    "    list_userPaperLibrary = list(df_UserRecord.collect()[0][2])\n",
    "    df_DeltaPapers = df_lda_paper_topic_model.filter(~df_lda_paper_topic_model.id.isin(list_userPaperLibrary)).selectExpr(\"id AS paper_id\",\"topicDistribution\")\n",
    "    \n",
    "    #Repartioning the data\n",
    "    df_UserRecord = df_UserRecord.repartition(40)\n",
    "    df_DeltaPapers = df_DeltaPapers.repartition(40)\n",
    "    \n",
    "    #Cross Join User with each paper\n",
    "    df_User_XJoin_Papers = df_UserRecord.selectExpr(\"user_hash_id\",\"sum_lda_vector AS user_lda_vector\").crossJoin(df_DeltaPapers)\n",
    "    \n",
    "    #Calculate the Cosine Similarity\n",
    "    df_cosine_similarity_result = df_User_XJoin_Papers.rdd.map(lambda x: (x[0],x[2],calculateCosineSimilarity(x[1],x[3]))).toDF(schema=['user_hash_id', 'paper_id', 'cosine_similarity_value'])\n",
    "    \n",
    "    #Fetch the Top K recommendations for the User\n",
    "    df_top_k_recommendations = df_cosine_similarity_result.orderBy(col(\"cosine_similarity_value\").desc()).limit(numberOfRecommendation)\n",
    "    #list_top_k_recommendations = list(df_top_k_recommendations.selectExpr(\"paper_id\").toPandas()['paper_id'])\n",
    "    df_top_k_recommendations = df_top_k_recommendations.groupBy(\"user_hash_id\").agg(collect_list(\"paper_id\").alias(\"recommended_library\"),collect_list(\"cosine_similarity_value\").alias(\"cosine_similarity_recommended_library\"))\n",
    "    \n",
    "    return df_top_k_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d9fa44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Ex 4.3 c) Top K recommendations for User = 1eac022a97d683eace8815545ce3153f\n",
    "\n",
    "user_hash_id = '1eac022a97d683eace8815545ce3153f'\n",
    "k = 10 #No Of Recommendation\n",
    "\n",
    "#Recommendation using TF-IDF Vector\n",
    "recommendation_with_tf_idf=tf_idf_CBRS(user_hash_id,k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d67931fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 349:==================================================>(1599 + 1) / 1600]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|user_hash_id                    |recommended_library                                                                       |cosine_similarity_recommended_library                                                                                                                                                                 |\n",
      "+--------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1eac022a97d683eace8815545ce3153f|[854469, 2838456, 4209212, 11692301, 10100718, 6306064, 7326487, 2284574, 3374934, 940716]|[0.5651574371602972, 0.553973821716132, 0.5522659264028578, 0.5319656547681235, 0.5257845041620104, 0.5212552998455351, 0.519000678739939, 0.5105764737972462, 0.5096110833325538, 0.5030455381239856]|\n",
      "+--------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Top 10 Recommendation for User:1eac022a97d683eace8815545ce3153f with TF-IDF \n",
    "recommendation_with_tf_idf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1c7baaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Recommendation using LDA vector\n",
    "recommendation_with_lda=lda_CBRS(user_hash_id,k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1d6cb1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 356:================================================> (1556 + 16) / 1600]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|user_hash_id                    |recommended_library                                                                 |cosine_similarity_recommended_library                                                                                                                                                                 |\n",
      "+--------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1eac022a97d683eace8815545ce3153f|[1667053, 398597, 1058069, 411197, 615335, 771028, 6505595, 620724, 351220, 5205986]|[0.8090922520253874, 0.808321118932079, 0.8048977696685601, 0.8011954110472813, 0.8008885855710791, 0.8007867105364813, 0.800075501748185, 0.7999333684798722, 0.7966779422001307, 0.7914643651173617]|\n",
      "+--------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Top 10 Recommendation for User:1eac022a97d683eace8815545ce3153f with LDA\n",
    "recommendation_with_lda.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cd0592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
