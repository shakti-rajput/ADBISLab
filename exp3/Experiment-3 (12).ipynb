{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0a96e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SparkSession,SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import StopWordsRemover, RegexTokenizer,HashingTF\n",
    "from pyspark.ml import Pipeline\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.clustering import LDA\n",
    "from pyspark.ml.feature import IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e28eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Spark Session\n",
    "spark = SparkSession.builder.appName(\"Experiment3\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "10e53038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Keywords dataframe\\n\\nkeywordsSchema = StructType([\\n    StructField(\"paper_id\",StringType(),False),\\n    StructField(\"keyword\",StringType(),False)\\n])\\n\\ndf_keywords = spark.read.csv(\"Datasets/keywords.csv\", sep = \",\", header = True, schema = keywordsSchema, quote = \\'\"\\')\\n\\n#Terms Dataframe\\nterms_df = spark.read.csv(\"Datasets/terms.txt\", header = True)\\n\\n#Stopword Broadcast\\nstopWords = sc.textFile(\"Datasets/stopwords_en.txt\")\\nstopWordsBroadcast = sc.broadcast(stopWords.collect())\\n\\n#Papers Terms Dataframe\\ndef parse_papers_count(line):\\n\\n    if not line:\\n        return dict()\\n    papers_count_raw = line.split(\\' \\')\\n    papers_count = dict()\\n    for pcRaw in papers_count_raw:\\n        paper, count = pcRaw.split(\\':\\')\\n        papers_count[paper] = int(count)\\n    return papers_count\\n\\npapers_vocab = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"Datasets/papers_terms.txt\").rdd\\npapers_vocab = papers_vocab.map(lambda x: (x[0], parse_papers_count(x[1]))).toDF().selectExpr(\\'_1 AS paper_id\\',\\'_2 AS term_count\\')    \\n\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PREPARING DATAFRAMES FOR DATSETS\n",
    "\n",
    "#Authors Dataframe\n",
    "#df_authors = spark.read.csv(\"Datasets/authors.csv\", sep = \",\", header = True, quote = '\"')\n",
    "\n",
    "#PaperCsv dataframe\n",
    "papersCsvSchema = StructType([\n",
    "    StructField(\"paper_id\",StringType(),False),\n",
    "    StructField(\"type\",StringType(),False),\n",
    "    StructField(\"journal\",StringType(),False),\n",
    "    StructField(\"book_title\",StringType(),False),\n",
    "    StructField(\"series\",StringType(),False),\n",
    "    StructField(\"publisher\",StringType(),False),\n",
    "    StructField(\"pages\",StringType(),False),\n",
    "    StructField(\"volume\",StringType(),False),\n",
    "    StructField(\"number\",StringType(),False),\n",
    "    StructField(\"year\",StringType(),False),\n",
    "    StructField(\"month\",StringType(),False),\n",
    "    StructField(\"postedat\",StringType(),False),\n",
    "    StructField(\"address\",StringType(),False),\n",
    "    StructField(\"title\",StringType(),False),\n",
    "    StructField(\"abstract\",StringType(),False),\n",
    "])\n",
    "df_paperCsv = spark.read.csv(\"Datasets/papers.csv\", sep = \",\", header = False, schema = papersCsvSchema, quote = '\"')\n",
    "\n",
    "#UserLibrary dataframe\n",
    "userLibrarySchema = StructType([\n",
    "    StructField(\"user_hash_id\",StringType(),False),\n",
    "    StructField(\"user_library\",StringType(),False)\n",
    "])\n",
    "df_userLibrary = spark.read.csv(\"Datasets/users_libraries.txt\", sep = \";\", header = False, schema = userLibrarySchema)\n",
    "df_userLibrary = df_userLibrary.selectExpr(\"user_hash_id\",\"split(user_library,',') AS user_library\")\n",
    "\n",
    "\"\"\"\n",
    "#Keywords dataframe\n",
    "\n",
    "keywordsSchema = StructType([\n",
    "    StructField(\"paper_id\",StringType(),False),\n",
    "    StructField(\"keyword\",StringType(),False)\n",
    "])\n",
    "\n",
    "df_keywords = spark.read.csv(\"Datasets/keywords.csv\", sep = \",\", header = True, schema = keywordsSchema, quote = '\"')\n",
    "\n",
    "#Terms Dataframe\n",
    "terms_df = spark.read.csv(\"Datasets/terms.txt\", header = True)\n",
    "\n",
    "#Stopword Broadcast\n",
    "stopWords = sc.textFile(\"Datasets/stopwords_en.txt\")\n",
    "stopWordsBroadcast = sc.broadcast(stopWords.collect())\n",
    "\n",
    "#Papers Terms Dataframe\n",
    "def parse_papers_count(line):\n",
    "\n",
    "    if not line:\n",
    "        return dict()\n",
    "    papers_count_raw = line.split(' ')\n",
    "    papers_count = dict()\n",
    "    for pcRaw in papers_count_raw:\n",
    "        paper, count = pcRaw.split(':')\n",
    "        papers_count[paper] = int(count)\n",
    "    return papers_count\n",
    "\n",
    "papers_vocab = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"Datasets/papers_terms.txt\").rdd\n",
    "papers_vocab = papers_vocab.map(lambda x: (x[0], parse_papers_count(x[1]))).toDF().selectExpr('_1 AS paper_id','_2 AS term_count')    \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e41c0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning and Tokenizing the data\n",
    "def phraseTokenization(x):\n",
    "    rawPhrase = x[13] + \" \" + x[14] #concatenating title and abstract\n",
    "    rawPhrase = rawPhrase.replace(\"-\",\"\") #removing - from phrase\n",
    "    rawPhrase = rawPhrase.replace(\"_\",\"\") #removing _ from phrase\n",
    "    rawPhrase = rawPhrase.strip() #removing any trailing or leading whitespaces\n",
    "    \n",
    "    #spliting phrase based on non-alphaNumeric characters\n",
    "    phraseArray = re.split('[^a-zA-Z0-9]+',rawPhrase) \n",
    "    \n",
    "    #remove words with less than 3 char\n",
    "    phraseArrayFilteredWords = [i for i in phraseArray if len(i) >= 3]\n",
    "    \n",
    "    return (x[0],list(phraseArrayFilteredWords))\n",
    "\n",
    "\n",
    "df_tokenize = df_paperCsv.na.fill(value=\"\").rdd.map(phraseTokenization).toDF()\n",
    "\n",
    "#Removing StopWords using ML\n",
    "swRemover = StopWordsRemover(inputCol=\"_2\", outputCol=\"cleaned_terms\")\n",
    "df_cleanedData = swRemover.transform(df_tokenize)\n",
    "df_cleanedData = df_cleanedData.selectExpr(\"_1 AS paper_id\",\"cleaned_terms\")\n",
    "\n",
    "#Stemming using Porter stemmer Algo\n",
    "ps =  PorterStemmer()\n",
    "\n",
    "def stemmingTerms(x):\n",
    "    stemmedWords = []\n",
    "    for word in x:\n",
    "        rootWord = ps.stem(word)\n",
    "        stemmedWords.append(rootWord)\n",
    "    return stemmedWords\n",
    "\n",
    "df_cleanedData = df_cleanedData.rdd.mapValues(stemmingTerms).toDF().selectExpr(\"_1 AS paper_id\",\"_2 AS cleaned_terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71118f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the count of papers in which the term is present\n",
    "df_paperCount = df_cleanedData.selectExpr(\"paper_id\",\"explode(cleaned_terms) AS terms\").distinct().groupBy(\"terms\").count().withColumnRenamed(\"count\", \"paper_count\")\n",
    "\n",
    "#10 percent of total papers present in file\n",
    "noOfDistinctPapers_df = int(df_cleanedData.select(countDistinct(\"paper_id\")).collect()[0][0])\n",
    "tenPercentOfTotalPapers = int(noOfDistinctPapers_df/10)\n",
    "\n",
    "# remove words appear in more than 10% of the papers and keep only the words that appear in at least 20 papers \n",
    "df_filterdTerms = df_paperCount.filter((df_paperCount[\"paper_count\"]<=tenPercentOfTotalPapers) & (df_paperCount[\"paper_count\"]>=20))\n",
    "\n",
    "#Fetch top 1000 terms \n",
    "top1000Terms = df_filterdTerms.orderBy(col(\"paper_count\").desc()).limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8621d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#associate unique integer values to each term\n",
    "df_termsWithUniqueIndex = top1000Terms.withColumn(\"unique_index\",row_number().over(Window.orderBy(\"paper_count\"))).selectExpr(\"terms\",\"unique_index-1 AS unique_index\")\n",
    "\n",
    "#Collect all terms in a list\n",
    "terms_collection =  [int(row.terms) for row in df_termsWithUniqueIndex.collect()]\n",
    "\n",
    "# Generating Termfrequency Vector for each paper\n",
    "df_cleanedDataExplode = df_cleanedData.selectExpr(\"paper_id\",\"explode(cleaned_terms) AS terms\")\n",
    "\n",
    "#Getting the Unique_index of term \n",
    "df_cleanedDataJoinIndex = df_cleanedDataExplode.join(df_termsWithUniqueIndex,df_cleanedDataExplode.terms == df_termsWithUniqueIndex.terms , how = \"inner\").select(df_cleanedDataExplode.paper_id,df_cleanedDataExplode.terms,df_termsWithUniqueIndex.unique_index)\n",
    "\n",
    "#Getting the term_frequency in each paper\n",
    "df_cleanedDataJoinIndex = df_cleanedDataJoinIndex.groupBy(\"paper_id\",\"unique_index\").count().withColumnRenamed(\"count\", \"term_frquency\")\n",
    "\n",
    "#Creating a sparseVector respresentation for each paper\n",
    "rdd_CleanedDataReducedByPaperId = df_cleanedDataJoinIndex.rdd.map(lambda x: (x[0], [(x[1], x[2])])).reduceByKey(lambda a, b: a + b)\n",
    "rdd_CleanedDataReducedByPaperId = rdd_CleanedDataReducedByPaperId.map(lambda x: (x[0],Vectors.sparse(1000,x[1])))\n",
    "\n",
    "df_CleanedDataSparseVector = rdd_CleanedDataReducedByPaperId.toDF().selectExpr(\"_1 AS paper_id\",\"_2 AS term_frequency_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b89110e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(paper_id='498902', term_frequency_vector=SparseVector(1000, {33: 3.0, 47: 1.0, 79: 1.0, 97: 1.0, 138: 1.0, 170: 6.0, 354: 1.0, 368: 1.0, 394: 1.0, 482: 2.0, 491: 1.0, 541: 1.0, 550: 1.0, 566: 1.0, 581: 1.0, 596: 1.0, 622: 1.0, 632: 1.0, 663: 1.0, 670: 1.0, 720: 2.0, 723: 1.0, 762: 1.0, 764: 1.0, 773: 1.0, 797: 1.0, 820: 1.0, 826: 1.0, 837: 2.0, 843: 1.0, 879: 1.0, 881: 1.0, 890: 1.0, 892: 1.0, 894: 1.0, 928: 1.0, 937: 1.0, 946: 2.0, 949: 3.0, 952: 1.0, 962: 1.0, 965: 1.0, 984: 1.0, 985: 4.0, 990: 1.0, 992: 1.0}))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_CleanedDataSparseVector.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e81d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF Representation for each paper\n",
    "\n",
    "idf = IDF(inputCol=\"term_frequency_vector\", outputCol=\"tf_idf_vector\")\n",
    "tf_idf_model = idf.fit(df_CleanedDataSparseVector)\n",
    "df_rescaledCleanedData = tf_idf_model.transform(df_CleanedDataSparseVector)\n",
    "df_rescaledCleanedData = df_rescaledCleanedData.select(\"paper_id\", \"tf_idf_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a942aa94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(paper_id='498902', tf_idf_vector=SparseVector(1000, {33: 13.8439, 47: 4.596, 79: 4.5494, 97: 4.5325, 138: 4.4866, 170: 26.6474, 354: 4.1522, 368: 4.1302, 394: 4.0846, 482: 7.8051, 491: 3.8889, 541: 3.7983, 550: 3.7834, 566: 3.7571, 581: 3.7152, 596: 3.6986, 622: 3.6484, 632: 3.6256, 663: 3.5658, 670: 3.5395, 720: 6.7871, 723: 3.3875, 762: 3.2929, 764: 3.2917, 773: 3.2717, 797: 3.2131, 820: 3.1488, 826: 3.1306, 837: 6.153, 843: 3.0651, 879: 2.9588, 881: 2.9516, 890: 2.9076, 892: 2.9043, 894: 2.8955, 928: 2.7393, 937: 2.6945, 946: 5.2459, 949: 7.8223, 952: 2.5913, 962: 2.5231, 965: 2.5148, 984: 2.3717, 985: 9.4584, 990: 2.328, 992: 2.3177}))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rescaledCleanedData.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc99f8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Direchlet Allocation\n",
    "\n",
    "num_topics = 40\n",
    "\n",
    "# Transform data into LDA supported format\n",
    "rdd_lda_format = df_CleanedDataSparseVector.rdd.mapValues(Vectors.fromML).map(lambda x: [int(x[0]),x[1]])\n",
    "\n",
    "#Train the LDA Model\n",
    "lda_model = LDA.train(rdd_lda_format, k=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe752359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the top 5 term of each extracted Topic\n",
    "topicIndices = sc.parallelize(lda_model.describeTopics(maxTermsPerTopic = 5))\n",
    "\n",
    "def Get_TopicTerms(topic):\n",
    "    termsId_array = topic[0] \n",
    "    terms_array = []\n",
    "    \n",
    "    #Finding the term from the their corresponding Unique index\n",
    "    for i in range(5):\n",
    "        term = terms_collection[termsId_array[i]]\n",
    "        terms_array.append(term)\n",
    "    return terms_array\n",
    "\n",
    "topics_Final = topicIndices.map(Get_TopicTerms).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51301aa0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['gene', 'protein', 'network', 'interact', 'activ'],\n",
       " ['network', 'gene', 'protein', 'interact', 'activ'],\n",
       " ['gene', 'protein', 'network', 'sequenc', 'cell'],\n",
       " ['gene', 'protein', 'network', 'sequenc', 'interact'],\n",
       " ['gene', 'protein', 'network', 'cell', 'sequenc'],\n",
       " ['gene', 'protein', 'network', 'cell', 'sequenc'],\n",
       " ['gene', 'protein', 'network', 'sequenc', 'cell'],\n",
       " ['gene', 'network', 'protein', 'sequenc', 'cell'],\n",
       " ['gene', 'protein', 'network', 'cell', 'activ'],\n",
       " ['gene', 'protein', 'network', 'cell', 'activ'],\n",
       " ['gene', 'network', 'protein', 'sequenc', 'interact'],\n",
       " ['gene', 'network', 'protein', 'activ', 'sequenc'],\n",
       " ['gene', 'network', 'protein', 'interact', 'activ'],\n",
       " ['gene', 'protein', 'network', 'cell', 'activ'],\n",
       " ['network', 'gene', 'protein', 'interact', 'activ'],\n",
       " ['gene', 'network', 'protein', 'activ', 'interact'],\n",
       " ['gene', 'network', 'protein', 'cell', 'activ'],\n",
       " ['gene', 'network', 'protein', 'cell', 'activ'],\n",
       " ['network', 'gene', 'protein', 'activ', 'interact'],\n",
       " ['protein', 'gene', 'network', 'interact', 'activ'],\n",
       " ['gene', 'protein', 'network', 'sequenc', 'activ'],\n",
       " ['gene', 'network', 'protein', 'sequenc', 'activ'],\n",
       " ['gene', 'network', 'protein', 'cell', 'activ'],\n",
       " ['gene', 'cell', 'protein', 'network', 'sequenc'],\n",
       " ['cell', 'gene', 'protein', 'network', 'activ'],\n",
       " ['gene', 'protein', 'network', 'cell', 'activ'],\n",
       " ['gene', 'protein', 'network', 'cell', 'activ'],\n",
       " ['gene', 'protein', 'network', 'cell', 'activ'],\n",
       " ['network', 'gene', 'protein', 'interact', 'activ'],\n",
       " ['gene', 'protein', 'network', 'activ', 'cell'],\n",
       " ['network', 'gene', 'protein', 'interact', 'activ'],\n",
       " ['gene', 'network', 'cell', 'protein', 'activ'],\n",
       " ['gene', 'protein', 'network', 'activ', 'interact'],\n",
       " ['gene', 'protein', 'network', 'cell', 'sequenc'],\n",
       " ['gene', 'network', 'protein', 'activ', 'interact'],\n",
       " ['gene', 'protein', 'network', 'sequenc', 'interact'],\n",
       " ['network', 'gene', 'protein', 'cell', 'activ'],\n",
       " ['network', 'protein', 'gene', 'activ', 'cell'],\n",
       " ['gene', 'protein', 'network', 'sequenc', 'interact'],\n",
       " ['network', 'gene', 'protein', 'activ', 'interact']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5acf0c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#User Profiling\n",
    "\n",
    "df_userLibrary_explode = df_userLibrary.selectExpr(\"user_hash_id\",\"explode(user_library) AS paper_id\")\n",
    "df_userJoined_TfIdf = df_userLibrary_explode.join(df_rescaledCleanedData,df_userLibrary_explode.paper_id == df_rescaledCleanedData.paper_id, how=\"inner\").select(df_userLibrary_explode.user_hash_id,df_userLibrary_explode.paper_id,df_rescaledCleanedData.tf_idf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2311125d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+\n",
      "|        user_hash_id|paper_id|       tf_idf_vector|\n",
      "+--------------------+--------+--------------------+\n",
      "|21cc6e1f99fae502d...|  498902|(1000,[33,47,79,9...|\n",
      "|cfaa315e63b14e6d3...|  498902|(1000,[33,47,79,9...|\n",
      "|9ccd57a8d991f3d7c...|  498902|(1000,[33,47,79,9...|\n",
      "|c8612ac693308780b...|  498902|(1000,[33,47,79,9...|\n",
      "|a73ea3f829fde4c26...|  498902|(1000,[33,47,79,9...|\n",
      "|93833fb937c3b5ecc...|  498902|(1000,[33,47,79,9...|\n",
      "|bbae15df551ed4a89...|  498902|(1000,[33,47,79,9...|\n",
      "|40ee6b4e4fefb7bd3...|  498902|(1000,[33,47,79,9...|\n",
      "|e8e0c7b9dd5799e58...|  498902|(1000,[33,47,79,9...|\n",
      "|6abfda34cd59fe0d3...|  498902|(1000,[33,47,79,9...|\n",
      "|b564d6be2c8e64259...|  498902|(1000,[33,47,79,9...|\n",
      "|6b6c8775389742588...|  498902|(1000,[33,47,79,9...|\n",
      "|c74188f8efedf5e7c...|  498902|(1000,[33,47,79,9...|\n",
      "|8f8b77f3ced188fcd...|  498902|(1000,[33,47,79,9...|\n",
      "|4bdfa5ef59ac63133...|  498902|(1000,[33,47,79,9...|\n",
      "|511b1eea8a8223a87...|  498902|(1000,[33,47,79,9...|\n",
      "|79ebc343c1d8d7890...|  498902|(1000,[33,47,79,9...|\n",
      "|9d600698f68d8d033...|  498902|(1000,[33,47,79,9...|\n",
      "|6738592573c24705e...|  498902|(1000,[33,47,79,9...|\n",
      "|9f3d0452aef17efd2...|  498902|(1000,[33,47,79,9...|\n",
      "+--------------------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_userJoined_TfIdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75f7724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
