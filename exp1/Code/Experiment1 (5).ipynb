{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3de4748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import csv\n",
    "from collections import Counter\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.functions import countDistinct\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd1d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "spark = SparkSession.builder.appName('Experiment1').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4252a37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time taken(in seconds) to create RDD's: 2.0260565280914307\n"
     ]
    }
   ],
   "source": [
    "## Ex 1.2\n",
    "start_time = time.time()\n",
    "\n",
    "#UserLibrary RDD\n",
    "userLibraryRdd = sc.textFile(\"Datasets/users_libraries.txt\")\n",
    "userLibraryRdd = userLibraryRdd.map(lambda line: line.split(\";\")).map(lambda line: (line[0],list(map(int,line[1].split(\",\")))))\n",
    "\n",
    "#print(\"UserLibrary RDD Start...\")\n",
    "#userLibraryRdd.take(10)\n",
    "#print(\"UserLibrary RDD End...\")\n",
    "\n",
    "#PaperCsv RDD\n",
    "def processPaperCsv(line):\n",
    "    paperCsv = csv.reader([line.replace(\"\\0\", \"\")], delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "    paperCsvList = next(paperCsv)\n",
    "    return paperCsvList[0], paperCsvList[14]\n",
    "\n",
    "paperCsvRdd = sc.textFile(\"Datasets/papers.csv\")\n",
    "paperCsvRdd = paperCsvRdd.map(processPaperCsv).filter(lambda x: (x[1] != \"\" and x[1] != \" \")).map(lambda x: (int(x[0]),x[1].split(\" \")))\n",
    "\n",
    "#print(\"Paper Csv RDD Start...\")\n",
    "#paperCsvRdd.take(10)\n",
    "#print(\"Paper Csv RDD End...\")\n",
    "\n",
    "#Stopword Broadcast\n",
    "stopWords = sc.textFile(\"Datasets/stopwords_en.txt\")\n",
    "stopWordsBroadcast = sc.broadcast(stopWords.collect())\n",
    "\n",
    "#print(\"Stop words Start...\")\n",
    "#stopWordsBroadcast.value\n",
    "#print(\"Stop words End...\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Execution Time taken(in seconds) to create RDD's:\", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9dbc1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time taken(in seconds) for Joining collections and writing to file: 3594.6526041030884\n"
     ]
    }
   ],
   "source": [
    "##Ex1.3\n",
    "start_time = time.time()\n",
    "\n",
    "#Removing stopWords\n",
    "def removeStopWords(wordList):\n",
    "    abstractwordsList = wordList.copy()\n",
    "    for a in wordList:\n",
    "        if ((a in stopWordsBroadcast.value) or a == \"\" or a == \" \"):\n",
    "            abstractwordsList.remove(a)\n",
    "    return abstractwordsList\n",
    "\n",
    "userLibraryJoinPaperRdd = userLibraryRdd.flatMapValues(lambda x: x).map(lambda x: (x[1],x[0])).join(paperCsvRdd)\n",
    "userLibraryJoinPaperRdd = userLibraryJoinPaperRdd.map(lambda x: (x[1][0],x[1][1]))\n",
    "userLibraryJoinPaperRdd = userLibraryJoinPaperRdd.flatMapValues(lambda x:x).groupByKey().mapValues(list)\n",
    "userLibraryJoinPaperWithoutStopWordsRdd = userLibraryJoinPaperRdd.mapValues(removeStopWords)\n",
    "\n",
    "#Finding top 10 most frequent words\n",
    "def findTopMostFrequentWords(x):\n",
    "    CounterList = Counter(x)\n",
    "    topTenMostFrequentWordWithCount = CounterList.most_common(10)\n",
    "    topTenMostFrequentWord = [word for word, word_count in topTenMostFrequentWordWithCount]\n",
    "    return topTenMostFrequentWord\n",
    "\n",
    "frequentlyOccuringWordList = userLibraryJoinPaperWithoutStopWordsRdd.mapValues(findTopMostFrequentWords)\n",
    "\n",
    "#wrtiting data to file\n",
    "def CreateCsvLine(data):\n",
    "    csvLineData = data[0] + \",\" + (','.join(str(d) for d in data[1]))\n",
    "    return csvLineData\n",
    "\n",
    "frequentlyOccuringWordListFile = frequentlyOccuringWordList.map(CreateCsvLine)\n",
    "frequentlyOccuringWordListFile.saveAsTextFile(\"Outputs/Top10WordsForEachUser_RDD\")\n",
    "\n",
    "#print(\"Top 10 most frequent words of each user_hash_id Start...\")\n",
    "#frequentlyOccuringWordListFile.take(10)\n",
    "#print(\"Top 10 most frequent words of each user_hash_id End...\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Execution Time taken(in seconds) for Joining collections and writing to file:\", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a61bcf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of (distinct) user: 28416\n",
      "Number of (distinct) items: 172079\n",
      "Number of ratings: 828481\n",
      "Min number of ratings a user has given: 1\n",
      "Max number of ratings a user has given: 1922\n",
      "Average number of ratings of users: 29.155440596846848\n",
      "Standard deviation for ratings of users: 81.1751761366871\n",
      "Min number of ratings an item has received: 3\n",
      "Max number of ratings an item has received: 924\n",
      "Average number of ratings of items: 4.81453867119172\n",
      "Standard deviation for ratings of items: 5.477802292314525\n",
      "Execution Time taken(in seconds) for Basic Analysis for Recommender Systems: 9.11683988571167\n"
     ]
    }
   ],
   "source": [
    "#Ex1.4\n",
    "start_time = time.time()\n",
    "\n",
    "#a\n",
    "userLibrayFMVRdd =userLibraryRdd.flatMapValues(lambda x:x)\n",
    "\n",
    "noOfDistinctUsers = userLibrayFMVRdd.keys().distinct().count()\n",
    "noOfDistinctItems = userLibrayFMVRdd.values().distinct().count()\n",
    "noOfRatings = userLibrayFMVRdd.values().count()\n",
    "\n",
    "#b,c,d,e\n",
    "ratingsList = userLibraryRdd.map(lambda x: (x[0],len(x[1]))).map(lambda x: x[1])\n",
    "\n",
    "minNoOfRatingUserHasGiven = ratingsList.min()\n",
    "#minNoOfRatingUserHasGiven = userLibraryRdd.map(lambda x: (x[0],len(x[1]))).sortBy(lambda x: x[1], ascending=True).map(lambda x: x[1]).first()\n",
    "maxNoOfRatingUserHasGiven = ratingsList.max()\n",
    "#maxNoOfRatingUserHasGiven = userLibraryRdd.map(lambda x: (x[0],len(x[1]))).sortBy(lambda x: x[1], ascending=False).map(lambda x: x[1]).first()\n",
    "avgNumberOfRatingUserGave = noOfRatings/noOfDistinctUsers\n",
    "standardDeviationOfRating = ratingsList.stdev()\n",
    "\n",
    "#f,g,h,i\n",
    "userLibraryReduceByPaperIdRdd = userLibrayFMVRdd.map(lambda x: (x[1],1)).reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "ratingsListByPaperId = userLibraryReduceByPaperIdRdd.map(lambda x: x[1])\n",
    "minNoOfRatingItemHasReceived = ratingsListByPaperId.min()\n",
    "maxNoOfRatingItemHasReceived = ratingsListByPaperId.max()\n",
    "avgNumberOfRatingOfItems = noOfRatings/noOfDistinctItems\n",
    "standardDeviationOfRItem = ratingsListByPaperId.stdev()\n",
    "\n",
    "print(\"Number of (distinct) user:\" ,noOfDistinctUsers)\n",
    "print(\"Number of (distinct) items:\" ,noOfDistinctItems)\n",
    "print(\"Number of ratings:\" ,noOfRatings)\n",
    "print(\"Min number of ratings a user has given:\",minNoOfRatingUserHasGiven)\n",
    "print(\"Max number of ratings a user has given:\",maxNoOfRatingUserHasGiven)\n",
    "print(\"Average number of ratings of users:\",avgNumberOfRatingUserGave)\n",
    "print(\"Standard deviation for ratings of users:\",standardDeviationOfRating)\n",
    "print(\"Min number of ratings an item has received:\",minNoOfRatingItemHasReceived)\n",
    "print(\"Max number of ratings an item has received:\",maxNoOfRatingItemHasReceived)\n",
    "print(\"Average number of ratings of items:\",avgNumberOfRatingOfItems)\n",
    "print(\"Standard deviation for ratings of items:\",standardDeviationOfRItem)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Execution Time taken(in seconds) for Basic Analysis for Recommender Systems:\", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b99d5b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time taken(in seconds) to create Dataframes: 2.894200325012207\n"
     ]
    }
   ],
   "source": [
    "#Ex1.5\n",
    "start_time = time.time()\n",
    "\n",
    "#UserLibrary dataframe\n",
    "userLibrarySchema = StructType([\n",
    "    StructField(\"user_hash_id\",StringType(),False),\n",
    "    StructField(\"user_library\",StringType(),False)\n",
    "])\n",
    "df_userLibrary = spark.read.csv(\"Datasets/users_libraries.txt\", sep = \";\", header = False, schema = userLibrarySchema)\n",
    "df_userLibrary = df_userLibrary.selectExpr(\"user_hash_id\",\"split(user_library,',') AS user_library\")\n",
    "\n",
    "#print(\"UserLibrary Dataframe Start...\")\n",
    "#df_userLibrary.take(10)\n",
    "#print(\"UserLibrary Dataframe End...\")\n",
    "\n",
    "#PaperCsv dataframe\n",
    "papersCsvSchema = StructType([\n",
    "    StructField(\"paper_id\",StringType(),False),\n",
    "    StructField(\"type\",StringType(),False),\n",
    "    StructField(\"journal\",StringType(),False),\n",
    "    StructField(\"book_title\",StringType(),False),\n",
    "    StructField(\"series\",StringType(),False),\n",
    "    StructField(\"publisher\",StringType(),False),\n",
    "    StructField(\"pages\",StringType(),False),\n",
    "    StructField(\"volume\",StringType(),False),\n",
    "    StructField(\"number\",StringType(),False),\n",
    "    StructField(\"year\",StringType(),False),\n",
    "    StructField(\"month\",StringType(),False),\n",
    "    StructField(\"postedat\",StringType(),False),\n",
    "    StructField(\"address\",StringType(),False),\n",
    "    StructField(\"title\",StringType(),False),\n",
    "    StructField(\"abstract\",StringType(),False),\n",
    "])\n",
    "df_paperCsv = spark.read.csv(\"Datasets/papers.csv\", sep = \",\", header = False, schema = papersCsvSchema, quote = '\"')\n",
    "df_paperCsv = df_paperCsv.selectExpr(\"paper_id\",\"split(replace(abstract,'\\\"',''),' ') AS abstract\")\n",
    "df_paperCsv = df_paperCsv.na.drop(subset=[\"abstract\"])\n",
    "\n",
    "#print(\"Paper Csv Dataframe Start...\")\n",
    "#df_paperCsv.take(10)\n",
    "#print(\"Paper Csv Dataframe End...\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Execution Time taken(in seconds) to create Dataframes:\", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00cd32c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time taken(in seconds) for Joining collections and writing to file 319.82534885406494\n"
     ]
    }
   ],
   "source": [
    "#Ex1.6\n",
    "start_time = time.time()\n",
    "\n",
    "#Explode UserLibrary\n",
    "userLibraryExplode = df_userLibrary.select(df_userLibrary.user_hash_id,explode(df_userLibrary.user_library).alias(\"paper_id\"))\n",
    "\n",
    "#Join UserLibrary and PaperCsv\n",
    "df_userLibraryJoinPaperCsv = df_paperCsv.join(userLibraryExplode,df_paperCsv.paper_id == userLibraryExplode.paper_id, how=\"inner\").select(userLibraryExplode.user_hash_id,userLibraryExplode.paper_id,df_paperCsv.abstract)\n",
    "\n",
    "#Removing stop words\n",
    "df_userLibraryJoinPaperCsv = df_userLibraryJoinPaperCsv.select(df_userLibraryJoinPaperCsv.user_hash_id,explode(df_userLibraryJoinPaperCsv.abstract).alias(\"abstract\"))\n",
    "useless_words = ['',' ','\"']\n",
    "df_userLibraryJoinPaperCsvWithoutStopWords = df_userLibraryJoinPaperCsv[~df_userLibraryJoinPaperCsv[\"abstract\"].isin(stopWordsBroadcast.value)]\n",
    "df_userLibraryJoinPaperCsvWithoutStopWords = df_userLibraryJoinPaperCsvWithoutStopWords[~df_userLibraryJoinPaperCsvWithoutStopWords[\"abstract\"].isin(useless_words)]\n",
    "\n",
    "#Finding top 10 most frequent words\n",
    "df_userLibraryJoinPaperCsvWithoutStopWordsCount = df_userLibraryJoinPaperCsvWithoutStopWords.groupBy(\"user_hash_id\",\"abstract\").count().withColumnRenamed(\"count\", \"word_count\")\n",
    "userWords_window = Window.partitionBy(df_userLibraryJoinPaperCsvWithoutStopWordsCount.user_hash_id).orderBy(col(\"word_count\").desc())\n",
    "df_userLibraryJoinPaperCsvWithoutStopWordsRank = df_userLibraryJoinPaperCsvWithoutStopWordsCount.withColumn(\"word_rank\",rank().over(userWords_window))\n",
    "df_topFrequentWordsPerUser = df_userLibraryJoinPaperCsvWithoutStopWordsRank.filter(df_userLibraryJoinPaperCsvWithoutStopWordsRank[\"word_rank\"]<11)\n",
    "df_groupedTop10FrequentWordsPerUser = df_topFrequentWordsPerUser.groupBy(\"user_hash_id\").agg(collect_list(\"abstract\")).withColumnRenamed(\"collect_list(abstract)\", \"abstract_word_list\")\n",
    "\n",
    "#writing top 10 most frequent words of each user to file\n",
    "df_groupedTop10FrequentWordsPerUser.write.save(\"Outputs/Top10WordsForEachUser_DF\")\n",
    "\n",
    "#print(\"Top 10 most frequent words of each user_hash_id Start...\")\n",
    "#df_groupedTop10FrequentWordsPerUser.take(10)\n",
    "#print(\"Top 10 most frequent words of each user_hash_id End...\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Execution Time taken(in seconds) for Joining collections and writing to file\", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca2a273a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of (distinct) user: 28416\n",
      "Number of (distinct) items: 172079\n",
      "Number of ratings: 828481\n",
      "Min number of ratings a user has given: 1\n",
      "Max number of ratings a user has given: 1922\n",
      "Average number of ratings of users: 29.155440596846848\n",
      "Standard deviation for ratings of users: 81.17660451011605\n",
      "Min number of ratings an item has received: 3\n",
      "Max number of ratings an item has received: 924\n",
      "Average number of ratings of items: 4.81453867119172\n",
      "Standard deviation for ratings of items: 5.477818208917284\n",
      "Execution Time taken(in seconds) for Basic Analysis for Recommender Systems: 15.048475742340088\n"
     ]
    }
   ],
   "source": [
    "#Ex1.6\n",
    "start_time = time.time()\n",
    "\n",
    "#a\n",
    "noOfDistinctUsers_df = str(userLibraryExplode.select(countDistinct(\"user_hash_id\")).collect()[0][0])\n",
    "noOfDistinctItems_df = str(userLibraryExplode.select(countDistinct(\"paper_id\")).collect()[0][0])\n",
    "noOfRatings_df = userLibraryExplode.count()\n",
    "\n",
    "#b,c,d,e\n",
    "ratingsList_df = userLibraryExplode.groupBy(\"user_hash_id\").count().withColumnRenamed(\"count\",\"no_of_items\")\n",
    "ratingsList_df = ratingsList_df.describe(\"no_of_items\")\n",
    "\n",
    "minNoOfRatingUserHasGiven = str(ratingsList_df.filter(\"summary == 'min'\").collect()[0][1])\n",
    "maxNoOfRatingUserHasGiven = str(ratingsList_df.filter(\"summary == 'max'\").collect()[0][1])\n",
    "avgNumberOfRatingUserGave = int(noOfRatings_df)/int(noOfDistinctUsers_df)\n",
    "standardDeviationOfRating = str(ratingsList_df.filter(\"summary == 'stddev'\").collect()[0][1])\n",
    "\n",
    "#f,g,h,i\n",
    "ratingsListByPaperId_df = userLibraryExplode.groupBy(\"paper_id\").count().withColumnRenamed(\"count\",\"no_of_ratings\")\n",
    "ratingsListByPaperId_df = ratingsListByPaperId_df.describe(\"no_of_ratings\")\n",
    "\n",
    "minNoOfRatingItemHasReceived_df = str(ratingsListByPaperId_df.filter(\"summary == 'min'\").collect()[0][1])\n",
    "maxNoOfRatingItemHasReceived_df = str(ratingsListByPaperId_df.filter(\"summary == 'max'\").collect()[0][1])\n",
    "avgNumberOfRatingOfItems_df = int(noOfRatings_df)/int(noOfDistinctItems_df)\n",
    "standardDeviationOfRItem_df = str(ratingsListByPaperId_df.filter(\"summary == 'stddev'\").collect()[0][1])\n",
    "\n",
    "print(\"Number of (distinct) user:\" ,noOfDistinctUsers_df)\n",
    "print(\"Number of (distinct) items:\" ,noOfDistinctItems_df)\n",
    "print(\"Number of ratings:\" ,noOfRatings_df)\n",
    "print(\"Min number of ratings a user has given:\",minNoOfRatingUserHasGiven)\n",
    "print(\"Max number of ratings a user has given:\",maxNoOfRatingUserHasGiven)\n",
    "print(\"Average number of ratings of users:\",avgNumberOfRatingUserGave)\n",
    "print(\"Standard deviation for ratings of users:\",standardDeviationOfRating)\n",
    "print(\"Min number of ratings an item has received:\",minNoOfRatingItemHasReceived_df)\n",
    "print(\"Max number of ratings an item has received:\",maxNoOfRatingItemHasReceived_df)\n",
    "print(\"Average number of ratings of items:\",avgNumberOfRatingOfItems_df)\n",
    "print(\"Standard deviation for ratings of items:\",standardDeviationOfRItem_df)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Execution Time taken(in seconds) for Basic Analysis for Recommender Systems:\", end_time-start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
