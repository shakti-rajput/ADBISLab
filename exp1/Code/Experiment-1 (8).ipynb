{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3de4748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import csv\n",
    "from collections import Counter\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd1d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "spark = SparkSession.builder.appName('Experiment1').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4252a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "userLibraryRdd = sc.textFile(\"Datasets/users_libraries.txt\")\n",
    "userLibraryRdd = userLibraryRdd.map(lambda line: line.split(\";\")).map(lambda line: (line[0],list(map(int,line[1].split(\",\")))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7df967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processPaperCsv(line):\n",
    "    paperCsv = csv.reader([line.replace(\"\\0\", \"\")], delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "    paperCsvList = next(paperCsv)\n",
    "    return paperCsvList[0], paperCsvList[14]\n",
    "\n",
    "paperCsvRdd = sc.textFile(\"Datasets/papers.csv\")\n",
    "paperCsvRdd = paperCsvRdd.map(processPaperCsv).filter(lambda x: (x[1] != \"\" and x[1] != \" \")).map(lambda x: (int(x[0]),x[1].split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66cd7c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = sc.textFile(\"Datasets/stopwords_en.txt\")\n",
    "stopWordsBroadcast = sc.broadcast(stopWords.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9dbc1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(wordList):\n",
    "    abstractwordsList = wordList.copy()\n",
    "    for a in wordList:\n",
    "        if ((a in stopWordsBroadcast.value) or a == \"\" or a == \" \"):\n",
    "            abstractwordsList.remove(a)\n",
    "    return abstractwordsList\n",
    "\n",
    "userLibraryJoinPaperRdd = userLibraryRdd.flatMapValues(lambda x: x).map(lambda x: (x[1],x[0])).join(paperCsvRdd)\n",
    "userLibraryJoinPaperRdd = userLibraryJoinPaperRdd.map(lambda x: (x[1][0],x[1][1]))\n",
    "userLibraryJoinPaperRdd = userLibraryJoinPaperRdd.flatMapValues(lambda x:x).groupByKey().mapValues(list)\n",
    "userLibraryJoinPaperWithoutStopWordsRdd = userLibraryJoinPaperRdd.mapValues(removeStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ecff4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTopMostFrequentWords(x):\n",
    "    CounterList = Counter(x)\n",
    "    topTenMostFrequentWordWithCount = CounterList.most_common(10)\n",
    "    topTenMostFrequentWord = [word for word, word_count in topTenMostFrequentWordWithCount]\n",
    "    return topTenMostFrequentWord\n",
    "\n",
    "frequentlyOccuringWordList = userLibraryJoinPaperWithoutStopWordsRdd.mapValues(findTopMostFrequentWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f522b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequentlyOccuringWordList.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ad71a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateCsvLine(data):\n",
    "    csvLineData = data[0] + \",\" + (','.join(str(d) for d in data[1]))\n",
    "    return csvLineData\n",
    "\n",
    "frequentlyOccuringWordListFile = frequentlyOccuringWordList.map(CreateCsvLine)\n",
    "frequentlyOccuringWordListFile.saveAsTextFile(\"Datasets/Top10MostFrequentWordsForEachUser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61418db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequentlyOccuringWordListFile.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a61bcf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of (distinct) user: 28416\n",
      "Number of (distinct) items: 172079\n",
      "Number of ratings: 828481\n"
     ]
    }
   ],
   "source": [
    "#Ex1.4\n",
    "\n",
    "userLibrayFMVRdd =userLibraryRdd.flatMapValues(lambda x:x)\n",
    "\n",
    "#a\n",
    "noOfDistinctUsers = userLibrayFMVRdd.keys().distinct().count()\n",
    "noOfDistinctItems = userLibrayFMVRdd.values().distinct().count()\n",
    "noOfRatings = userLibrayFMVRdd.values().count()\n",
    "\n",
    "print(\"Number of (distinct) user:\" ,noOfDistinctUsers)\n",
    "print(\"Number of (distinct) items:\" ,noOfDistinctItems)\n",
    "print(\"Number of ratings:\" ,noOfRatings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13f541ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b,c,d,e\n",
    "\n",
    "ratingsList = userLibraryRdd.map(lambda x: (x[0],len(x[1]))).map(lambda x: x[1])\n",
    "minNoOfRatingUserHasGiven = ratingsList.min()\n",
    "#minNoOfRatingUserHasGiven = userLibraryRdd.map(lambda x: (x[0],len(x[1]))).sortBy(lambda x: x[1], ascending=True).map(lambda x: x[1]).first()\n",
    "maxNoOfRatingUserHasGiven = ratingsList.max()\n",
    "#maxNoOfRatingUserHasGiven = userLibraryRdd.map(lambda x: (x[0],len(x[1]))).sortBy(lambda x: x[1], ascending=False).map(lambda x: x[1]).first()\n",
    "avgNumberOfRatingUserGave = noOfRatings/noOfDistinctUsers\n",
    "standardDeviationOfRating = ratingsList.stdev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "940bc13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min number of ratings a user has given: 1\n",
      "Max number of ratings a user has given: 1922\n",
      "Average number of ratings of users: 29.155440596846848\n",
      "Standard deviation for ratings of users: 81.1751761366871\n"
     ]
    }
   ],
   "source": [
    "print(\"Min number of ratings a user has given:\",minNoOfRatingUserHasGiven)\n",
    "print(\"Max number of ratings a user has given:\",maxNoOfRatingUserHasGiven)\n",
    "print(\"Average number of ratings of users:\",avgNumberOfRatingUserGave)\n",
    "print(\"Standard deviation for ratings of users:\",standardDeviationOfRating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a7cd49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "userLibraryReduceByPaperIdRdd = userLibrayFMVRdd.map(lambda x: (x[1],1)).reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ec81573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f,g,h,i\n",
    "\n",
    "ratingsListByPaperId = userLibraryReduceByPaperIdRdd.map(lambda x: x[1])\n",
    "minNoOfRatingItemHasReceived = ratingsListByPaperId.min()\n",
    "maxNoOfRatingItemHasReceived = ratingsListByPaperId.max()\n",
    "avgNumberOfRatingOfItems = noOfRatings/noOfDistinctItems\n",
    "standardDeviationOfRItem = ratingsListByPaperId.stdev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "376bcbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min number of ratings an item has received: 3\n",
      "Max number of ratings an item has received: 924\n",
      "Average number of ratings of items: 4.81453867119172\n",
      "Standard deviation for ratings of items: 5.477802292314525\n"
     ]
    }
   ],
   "source": [
    "print(\"Min number of ratings an item has received:\",minNoOfRatingItemHasReceived)\n",
    "print(\"Max number of ratings an item has received:\",maxNoOfRatingItemHasReceived)\n",
    "print(\"Average number of ratings of items:\",avgNumberOfRatingOfItems)\n",
    "print(\"Standard deviation for ratings of items:\",standardDeviationOfRItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b99d5b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "userLibrarySchema = StructType([\n",
    "    StructField(\"user_hash_id\",StringType(),False),\n",
    "    StructField(\"user_library\",StringType(),False)\n",
    "])\n",
    "df_userLibrary = spark.read.csv(\"Datasets/users_libraries.txt\", sep = \";\", header = False, schema = userLibrarySchema)\n",
    "df_userLibrary = df_userLibrary.selectExpr(\"user_hash_id\",\"split(user_library,',') AS user_library\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba22f740",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|        user_hash_id|        user_library|\n",
      "+--------------------+--------------------+\n",
      "|28d3f81251d94b097...|[3929762, 503574,...|\n",
      "|d0c9aaa788153daea...|[2080631, 6343346...|\n",
      "|f05bcffe7951de9e5...|[1158654, 478707,...|\n",
      "|ca4f1ba4094011d9a...|            [278019]|\n",
      "|d1d41a15201915503...|[6610569, 6493797...|\n",
      "|f2f77383828ea6d39...|[943458, 238121, ...|\n",
      "|9c883d02115400f7b...|[3509971, 3509965...|\n",
      "|b656009a6efdc8b1a...|[771870, 181369, ...|\n",
      "|cf9c7f356092c34be...|             [90558]|\n",
      "|0f5cbb39410a9278f...|           [9344598]|\n",
      "|d85f7d83f27b3f533...|[7610843, 3633347...|\n",
      "|586c867a0688250ac...|[464760, 466011, ...|\n",
      "|10fdfaf945d5c27ad...|           [2010550]|\n",
      "|589b870a611c25fa9...|[1283233, 1305474...|\n",
      "|90f1a3e6fcdbf9bc5...|[115945, 11733005...|\n",
      "|7e070a9da96672e05...|           [1071959]|\n",
      "|3b715ebaf1f8f81a1...|[4119394, 3378798...|\n",
      "|488fb15e8c77f8054...|[1523301, 5281566...|\n",
      "|3fdf355e59949c79d...|[7077220, 1289842...|\n",
      "|c6b59086a0bbac141...|[2230995, 3050075...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_userLibrary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d79c288",
   "metadata": {},
   "outputs": [],
   "source": [
    "papersCsvSchema = StructType([\n",
    "    StructField(\"paper_id\",StringType(),False),\n",
    "    StructField(\"type\",StringType(),False),\n",
    "    StructField(\"journal\",StringType(),False),\n",
    "    StructField(\"book_title\",StringType(),False),\n",
    "    StructField(\"series\",StringType(),False),\n",
    "    StructField(\"publisher\",StringType(),False),\n",
    "    StructField(\"pages\",StringType(),False),\n",
    "    StructField(\"volume\",StringType(),False),\n",
    "    StructField(\"number\",StringType(),False),\n",
    "    StructField(\"year\",StringType(),False),\n",
    "    StructField(\"month\",StringType(),False),\n",
    "    StructField(\"postedat\",StringType(),False),\n",
    "    StructField(\"address\",StringType(),False),\n",
    "    StructField(\"title\",StringType(),False),\n",
    "    StructField(\"abstract\",StringType(),False),\n",
    "])\n",
    "df_paperCsv = spark.read.csv(\"Datasets/papers.csv\", sep = \",\", header = False, schema = papersCsvSchema, quote = '\"')\n",
    "df_paperCsv = df_paperCsv.selectExpr(\"paper_id\",\"split(replace(abstract,'\\\"',''),' ') AS abstract\")\n",
    "df_paperCsv = df_paperCsv.na.drop(subset=[\"abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "695f3941",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'explode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-d6a840c99e00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_paperCsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"abstract\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \"\"\"\n\u001b[1;32m   1642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1643\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   1644\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[1;32m   1645\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'explode'"
     ]
    }
   ],
   "source": [
    "df_paperCsv.explode(\"abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17f0a077",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-c1b1a595c45e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_paperCsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremoveStopWords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \"\"\"\n\u001b[1;32m   1642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1643\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   1644\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[1;32m   1645\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "df_paperCsv.filter(\"abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7136c3d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
